{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tobigs_yookyung_week7_Framework_pytorch.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1k3rNwdmC2FxCEyeO7AOd_a8uSsM6FLqH","authorship_tag":"ABX9TyOn3AJb/NH5k7Rl8CjW0dpA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CBqvalCyQv_M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9be4af17-9af9-4f29-db05-5ad0c07e75f1","executionInfo":{"status":"ok","timestamp":1584445448183,"user_tz":-540,"elapsed":1183,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["#드라이브에 저장되어있는 데이터 불러오기\n","cd /content/drive/My Drive/Colab Notebooks/kaggle_dataset"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/kaggle_dataset\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6_hfOkGbQ10w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"fe02b576-c0c5-47d0-8f69-a89c85c41794","executionInfo":{"status":"ok","timestamp":1584445452746,"user_tz":-540,"elapsed":3984,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["ls -ltr"],"execution_count":2,"outputs":[{"output_type":"stream","text":["total 102857\n","-rw------- 1 root root   158651 Mar 13 04:02 sample_submission.csv\n","-rw------- 1 root root 31527613 Mar 13 04:02 test_df.csv\n","-rw------- 1 root root 73356919 Mar 13 04:03 train_df.csv\n","-rw------- 1 root root   140650 Mar 16 09:12 relu_adam_submission.csv\n","-rw------- 1 root root   140650 Mar 16 09:17 nobatchnorm_relu_adam_submission.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2q8z3vovQ4t_","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","sample_submission = pd.read_csv(\"sample_submission.csv\")\n","train = pd.read_csv(\"train_df.csv\")\n","test = pd.read_csv(\"test_df.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5YtqCqRQ-KV","colab_type":"code","colab":{}},"source":["#데이터 스케일링\n","X = train.iloc[:,1:].values / 255\n","y = train.iloc[:,0].values\n","X_test = test.iloc[:,1:].values / 255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZ7wptev1aLl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9a4994eb-1150-4dea-d8a9-24297ba1c31a","executionInfo":{"status":"ok","timestamp":1584445465641,"user_tz":-540,"elapsed":1385,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["#검증을 위해 validation dataset을 추가해보자\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=2020)\n","print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(33600, 784) (8400, 784) (33600,) (8400,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sn4GPBVQRB8E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5324adff-8d94-40a6-bf4a-569efcbc5e8e","executionInfo":{"status":"ok","timestamp":1584445469340,"user_tz":-540,"elapsed":1213,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["y_train"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 8, 8, ..., 6, 7, 8])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"uopjjlQPMNHp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bbff3062-1f53-4e81-bb71-b8ff0e24b48a","executionInfo":{"status":"ok","timestamp":1584443484479,"user_tz":-540,"elapsed":1127,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["y_val"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4, 9, 5, ..., 3, 4, 6])"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"v7VeRPyVby3v","colab_type":"text"},"source":["## DataLoader\n","모든 데이터를 나누고, 일일히 그 데이터를 나눠서 forward와 backward를 돌리는 식으로 진행해야 하는데, 파이토치에서 제공해주는 DataLoader을 쓴다면 그럴 필요가 없다. 그냥 DataLoader로 부터 batch_size만큼의 데이터를 받아오면 된다.\n","\n","출처: https://wingnim.tistory.com/33 [jinyo의 뇌]\n","\n","우리가 직접 만드는 custom dataloader은 다음과 같은 세 파트로 이루어져 있다.\n","\n","1. __init__(self)\n","\n","download, read data 등등을 하는 파트.\n","\n","\n","2. __getitem__(self,index)\n","\n","인덱스에 해당하는 아이템을 넘겨주는 파트.\n","\n","\n","3. __len__(self)\n","\n","data size를 넘겨주는 파트\n"]},{"cell_type":"code","metadata":{"id":"27rU5p-ZREH8","colab_type":"code","colab":{}},"source":["#Dataset과 Loader 정의\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, X, y, transform=None): #1. download, read data, etc\n","        self.X = X\n","        self.y = y\n","        self.transform = transform # 만약 이미지 데이터셋이라면 transform에 rotation, shear, crop 등등이 들어갈 수 있음\n","    \n","    def __getitem__(self, idx): #2. return one item on the index\n","        X = self.X[idx]\n","        y = self.y[idx]\n","        return X, y\n","\n","    def __len__(self): #3. return the data length\n","        return len(self.X) \n","\n","class ValidDataset(Dataset):\n","    def __init__(self, X, y, transform=None): #1. download, read data, etc\n","        self.X = X\n","        self.y = y\n","        self.transform = transform\n","    \n","    def __getitem__(self, idx): #2. return one item on the index\n","        X = self.X[idx]\n","        y = self.y[idx]\n","        return X,y\n","\n","    def __len__(self): #3. return the data length\n","        return len(self.X)    \n","\n","class TestDataset(Dataset):\n","    def __init__(self, X, y, transform=None): #1. download, read data, etc\n","        self.X = X\n","        self.y = y\n","        self.transform = transform\n","    \n","    def __getitem__(self, idx): #2. return one item on the index\n","        X = self.X[idx]\n","        return X\n","\n","    def __len__(self): #3. return the data length\n","        return len(self.X)        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MobwxjlfR7Nb","colab_type":"code","colab":{}},"source":["# train, valid, test dataloader 생성\n","#dataloader에서 batch_size 지정해주고 shuffle = True (train일 경우)\n","#test의 경우는 shuffle=False\n","traindataloader = DataLoader(TrainDataset(x_train, y_train), batch_size=128, shuffle=True, num_workers=4) #num_workers는 멀티쓰레딩을 지원하여 빠르게 데이터를 가져올 수 있다\n","validdataloader = DataLoader(ValidDataset(x_val, y_val), batch_size=64, shuffle=True, num_workers=4)\n","testdataloader = DataLoader(TestDataset(X_test, y=None), batch_size=4, shuffle=False, num_workers=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRIrstLKlXGK","colab_type":"code","colab":{}},"source":["# NN 모델 생성\n","# 각각 weight initializer(he), batch normalization, dropout(0.4 or 0.5) 에 변화를 줘가면서 3 layers or 4 layers로 구성했다. activation func은 relu로 통일\n","class Net(nn.Module):\n","  def __init__(self): \n","        super(Net, self).__init__()\n","        self.l1 = nn.Linear(784,512)\n","        #nn.init.kaiming_normal(self.l1.weight, nonlinearity='relu')\n","        self.bn1 = nn.BatchNorm1d(512)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        #self.drop1 = torch.nn.Dropout(p=0.4)\n","\n","        self.l2 = nn.Linear(512,256)\n","        #nn.init.kaiming_normal(self.l2.weight, nonlinearity='relu')\n","        self.bn2 = nn.BatchNorm1d(256)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        #self.drop2 = torch.nn.Dropout(p=0.4)\n","\n","        self.l3 = nn.Linear(256,128)\n","        #nn.init.kaiming_normal(self.l3.weight, nonlinearity='relu')\n","        self.bn3 = nn.BatchNorm1d(128)\n","        self.relu3 = nn.ReLU(inplace=True)\n","        #self.drop3 = torch.nn.Dropout(p=0.4)\n","\n","        self.l4 = nn.Linear(128,10)\n","        #nn.init.kaiming_normal(self.l4.weight, nonlinearity='relu')\n","\n","  def forward(self, x):\n","        x = self.l1(x)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","        #x = self.drop1(x)\n","        x = self.l2(x)\n","        x = self.bn2(x)\n","        x = self.relu2(x)\n","        #x = self.drop2(x)\n","        x = self.l3(x)\n","        x = self.bn3(x)\n","        x = self.relu3(x)\n","        #x = self.drop3(x)\n","        x = self.l4(x)\n","        ##소프트맥스 안걸어줘도된다\n","        #뒤에서 crossentropyloss 걸어주면 소프트맥스가 자동으로 적용된다고 한다\n","        return x\n","\n","net = Net()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0-CPoGVssvU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"41cbfbe5-b7d5-4d8e-bc68-54bf9e235a3d","executionInfo":{"status":"ok","timestamp":1584455089468,"user_tz":-540,"elapsed":1066,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["net"],"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (l1): Linear(in_features=784, out_features=512, bias=True)\n","  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu1): ReLU(inplace=True)\n","  (l2): Linear(in_features=512, out_features=256, bias=True)\n","  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu2): ReLU(inplace=True)\n","  (l3): Linear(in_features=256, out_features=128, bias=True)\n","  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu3): ReLU(inplace=True)\n","  (l4): Linear(in_features=128, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"cMyqzp1tsxnB","colab_type":"code","colab":{}},"source":["#Adam optimizer로 설정했다\n","import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=1e-3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBxFtG4sszno","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5fd8001d-1d18-41f9-f886-0208721e48fc","executionInfo":{"status":"ok","timestamp":1584455096635,"user_tz":-540,"elapsed":1044,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"95qC3OlIs9tt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"922aac18-0969-4bc3-8a55-a7302785f8a4","executionInfo":{"status":"ok","timestamp":1584455097762,"user_tz":-540,"elapsed":595,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["\n","net.to(device)"],"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (l1): Linear(in_features=784, out_features=512, bias=True)\n","  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu1): ReLU(inplace=True)\n","  (l2): Linear(in_features=512, out_features=256, bias=True)\n","  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu2): ReLU(inplace=True)\n","  (l3): Linear(in_features=256, out_features=128, bias=True)\n","  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu3): ReLU(inplace=True)\n","  (l4): Linear(in_features=128, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"4mfZ_o1Ls_lR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"18431ab6-747e-479a-bc2b-f31a8856f264","executionInfo":{"status":"ok","timestamp":1584452793187,"user_tz":-540,"elapsed":319411,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["# 3 layers, weight initialization(he), batch normalization, dropout(0.5)\n","trn_loss_list = []\n","val_loss_list = []\n","num_epochs = 100\n","num_batches = len(traindataloader)\n","for epoch in range(num_epochs):  # loop over the dataset multiple times\n","\n","    trn_loss = 0.0\n","    for i, data in enumerate(traindataloader):\n","        inputs, labels = data\n","        inputs = torch.tensor(inputs, device=device).float()\n","        labels = torch.tensor(labels, device=device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # print(type(inputs), type(labels), type(outputs))\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        # print(type(inputs), type(labels), type(outputs))\n","        loss = criterion(outputs, labels)\n","        # back propagation\n","        loss.backward()\n","        # weight update\n","        optimizer.step()\n","        # print statistics\n","        trn_loss += loss.item()\n","\n","        # del(memory issue)\n","        del loss\n","        del outputs\n","\n","        # 학습과정 출력\n","        if (i+1) % 100 == 0: # every 100 mini-batches\n","            with torch.no_grad(): # very very very very important!!!\n","                val_loss = 0.0\n","                for j, val in enumerate(validdataloader):\n","                    val_x, val_label = val\n","                    val_x = torch.tensor(val_x, device=device).float()\n","                    val_label = torch.tensor(val_label, device=device)\n","                    val_output = net(val_x)\n","                    v_loss = criterion(val_output, val_label)\n","                    val_loss += v_loss\n","                       \n","            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n","                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(validdataloader)\n","            ))            \n","            \n","            trn_loss_list.append(trn_loss/100)\n","            val_loss_list.append(val_loss/len(validdataloader))\n","            trn_loss = 0.0        \n","\n","       \n","\n","    #print(\"Epoch : {} loss: {}\".format(epoch, running_loss))\n","print('Finished Training')"],"execution_count":78,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 1/100 | step: 100/263 | trn loss: 2.4427 | val loss: 2.0571\n","epoch: 1/100 | step: 200/263 | trn loss: 1.8218 | val loss: 1.6573\n","epoch: 2/100 | step: 100/263 | trn loss: 1.3588 | val loss: 1.2812\n","epoch: 2/100 | step: 200/263 | trn loss: 1.1904 | val loss: 1.1574\n","epoch: 3/100 | step: 100/263 | trn loss: 1.0123 | val loss: 0.9711\n","epoch: 3/100 | step: 200/263 | trn loss: 0.9342 | val loss: 0.8999\n","epoch: 4/100 | step: 100/263 | trn loss: 0.8163 | val loss: 0.8030\n","epoch: 4/100 | step: 200/263 | trn loss: 0.7588 | val loss: 0.7489\n","epoch: 5/100 | step: 100/263 | trn loss: 0.6932 | val loss: 0.6835\n","epoch: 5/100 | step: 200/263 | trn loss: 0.6555 | val loss: 0.6604\n","epoch: 6/100 | step: 100/263 | trn loss: 0.5969 | val loss: 0.5976\n","epoch: 6/100 | step: 200/263 | trn loss: 0.5728 | val loss: 0.5704\n","epoch: 7/100 | step: 100/263 | trn loss: 0.5413 | val loss: 0.5292\n","epoch: 7/100 | step: 200/263 | trn loss: 0.5110 | val loss: 0.5075\n","epoch: 8/100 | step: 100/263 | trn loss: 0.4818 | val loss: 0.4860\n","epoch: 8/100 | step: 200/263 | trn loss: 0.4568 | val loss: 0.4632\n","epoch: 9/100 | step: 100/263 | trn loss: 0.4347 | val loss: 0.4423\n","epoch: 9/100 | step: 200/263 | trn loss: 0.4193 | val loss: 0.4182\n","epoch: 10/100 | step: 100/263 | trn loss: 0.3959 | val loss: 0.4291\n","epoch: 10/100 | step: 200/263 | trn loss: 0.3879 | val loss: 0.3994\n","epoch: 11/100 | step: 100/263 | trn loss: 0.3799 | val loss: 0.3813\n","epoch: 11/100 | step: 200/263 | trn loss: 0.3597 | val loss: 0.3777\n","epoch: 12/100 | step: 100/263 | trn loss: 0.3471 | val loss: 0.3679\n","epoch: 12/100 | step: 200/263 | trn loss: 0.3524 | val loss: 0.3538\n","epoch: 13/100 | step: 100/263 | trn loss: 0.3256 | val loss: 0.3475\n","epoch: 13/100 | step: 200/263 | trn loss: 0.3302 | val loss: 0.3403\n","epoch: 14/100 | step: 100/263 | trn loss: 0.3108 | val loss: 0.3330\n","epoch: 14/100 | step: 200/263 | trn loss: 0.3007 | val loss: 0.3261\n","epoch: 15/100 | step: 100/263 | trn loss: 0.3054 | val loss: 0.3207\n","epoch: 15/100 | step: 200/263 | trn loss: 0.2878 | val loss: 0.3198\n","epoch: 16/100 | step: 100/263 | trn loss: 0.2778 | val loss: 0.3193\n","epoch: 16/100 | step: 200/263 | trn loss: 0.2740 | val loss: 0.2906\n","epoch: 17/100 | step: 100/263 | trn loss: 0.2687 | val loss: 0.3036\n","epoch: 17/100 | step: 200/263 | trn loss: 0.2683 | val loss: 0.2877\n","epoch: 18/100 | step: 100/263 | trn loss: 0.2595 | val loss: 0.2882\n","epoch: 18/100 | step: 200/263 | trn loss: 0.2543 | val loss: 0.2847\n","epoch: 19/100 | step: 100/263 | trn loss: 0.2489 | val loss: 0.2921\n","epoch: 19/100 | step: 200/263 | trn loss: 0.2600 | val loss: 0.2718\n","epoch: 20/100 | step: 100/263 | trn loss: 0.2428 | val loss: 0.2598\n","epoch: 20/100 | step: 200/263 | trn loss: 0.2355 | val loss: 0.2665\n","epoch: 21/100 | step: 100/263 | trn loss: 0.2392 | val loss: 0.2725\n","epoch: 21/100 | step: 200/263 | trn loss: 0.2303 | val loss: 0.2664\n","epoch: 22/100 | step: 100/263 | trn loss: 0.2265 | val loss: 0.2621\n","epoch: 22/100 | step: 200/263 | trn loss: 0.2296 | val loss: 0.2522\n","epoch: 23/100 | step: 100/263 | trn loss: 0.2271 | val loss: 0.2597\n","epoch: 23/100 | step: 200/263 | trn loss: 0.2159 | val loss: 0.2486\n","epoch: 24/100 | step: 100/263 | trn loss: 0.2131 | val loss: 0.2485\n","epoch: 24/100 | step: 200/263 | trn loss: 0.2151 | val loss: 0.2487\n","epoch: 25/100 | step: 100/263 | trn loss: 0.2130 | val loss: 0.2500\n","epoch: 25/100 | step: 200/263 | trn loss: 0.2093 | val loss: 0.2446\n","epoch: 26/100 | step: 100/263 | trn loss: 0.2056 | val loss: 0.2431\n","epoch: 26/100 | step: 200/263 | trn loss: 0.2023 | val loss: 0.2376\n","epoch: 27/100 | step: 100/263 | trn loss: 0.2003 | val loss: 0.2305\n","epoch: 27/100 | step: 200/263 | trn loss: 0.2038 | val loss: 0.2408\n","epoch: 28/100 | step: 100/263 | trn loss: 0.2090 | val loss: 0.2247\n","epoch: 28/100 | step: 200/263 | trn loss: 0.1896 | val loss: 0.2387\n","epoch: 29/100 | step: 100/263 | trn loss: 0.1941 | val loss: 0.2245\n","epoch: 29/100 | step: 200/263 | trn loss: 0.1892 | val loss: 0.2229\n","epoch: 30/100 | step: 100/263 | trn loss: 0.1850 | val loss: 0.2206\n","epoch: 30/100 | step: 200/263 | trn loss: 0.1840 | val loss: 0.2156\n","epoch: 31/100 | step: 100/263 | trn loss: 0.1774 | val loss: 0.2244\n","epoch: 31/100 | step: 200/263 | trn loss: 0.1744 | val loss: 0.2262\n","epoch: 32/100 | step: 100/263 | trn loss: 0.1801 | val loss: 0.2164\n","epoch: 32/100 | step: 200/263 | trn loss: 0.1775 | val loss: 0.2213\n","epoch: 33/100 | step: 100/263 | trn loss: 0.1822 | val loss: 0.2227\n","epoch: 33/100 | step: 200/263 | trn loss: 0.1747 | val loss: 0.2212\n","epoch: 34/100 | step: 100/263 | trn loss: 0.1694 | val loss: 0.2248\n","epoch: 34/100 | step: 200/263 | trn loss: 0.1721 | val loss: 0.2140\n","epoch: 35/100 | step: 100/263 | trn loss: 0.1757 | val loss: 0.2074\n","epoch: 35/100 | step: 200/263 | trn loss: 0.1639 | val loss: 0.2115\n","epoch: 36/100 | step: 100/263 | trn loss: 0.1691 | val loss: 0.2245\n","epoch: 36/100 | step: 200/263 | trn loss: 0.1729 | val loss: 0.2143\n","epoch: 37/100 | step: 100/263 | trn loss: 0.1565 | val loss: 0.2056\n","epoch: 37/100 | step: 200/263 | trn loss: 0.1631 | val loss: 0.1975\n","epoch: 38/100 | step: 100/263 | trn loss: 0.1579 | val loss: 0.2055\n","epoch: 38/100 | step: 200/263 | trn loss: 0.1628 | val loss: 0.2022\n","epoch: 39/100 | step: 100/263 | trn loss: 0.1558 | val loss: 0.2023\n","epoch: 39/100 | step: 200/263 | trn loss: 0.1606 | val loss: 0.2024\n","epoch: 40/100 | step: 100/263 | trn loss: 0.1531 | val loss: 0.1970\n","epoch: 40/100 | step: 200/263 | trn loss: 0.1582 | val loss: 0.1973\n","epoch: 41/100 | step: 100/263 | trn loss: 0.1598 | val loss: 0.2003\n","epoch: 41/100 | step: 200/263 | trn loss: 0.1502 | val loss: 0.1981\n","epoch: 42/100 | step: 100/263 | trn loss: 0.1484 | val loss: 0.1929\n","epoch: 42/100 | step: 200/263 | trn loss: 0.1482 | val loss: 0.1958\n","epoch: 43/100 | step: 100/263 | trn loss: 0.1535 | val loss: 0.1952\n","epoch: 43/100 | step: 200/263 | trn loss: 0.1478 | val loss: 0.1783\n","epoch: 44/100 | step: 100/263 | trn loss: 0.1430 | val loss: 0.1912\n","epoch: 44/100 | step: 200/263 | trn loss: 0.1501 | val loss: 0.1915\n","epoch: 45/100 | step: 100/263 | trn loss: 0.1441 | val loss: 0.1861\n","epoch: 45/100 | step: 200/263 | trn loss: 0.1437 | val loss: 0.1969\n","epoch: 46/100 | step: 100/263 | trn loss: 0.1406 | val loss: 0.1917\n","epoch: 46/100 | step: 200/263 | trn loss: 0.1419 | val loss: 0.1936\n","epoch: 47/100 | step: 100/263 | trn loss: 0.1412 | val loss: 0.1908\n","epoch: 47/100 | step: 200/263 | trn loss: 0.1423 | val loss: 0.1919\n","epoch: 48/100 | step: 100/263 | trn loss: 0.1385 | val loss: 0.1847\n","epoch: 48/100 | step: 200/263 | trn loss: 0.1432 | val loss: 0.1867\n","epoch: 49/100 | step: 100/263 | trn loss: 0.1393 | val loss: 0.1805\n","epoch: 49/100 | step: 200/263 | trn loss: 0.1277 | val loss: 0.1831\n","epoch: 50/100 | step: 100/263 | trn loss: 0.1298 | val loss: 0.1873\n","epoch: 50/100 | step: 200/263 | trn loss: 0.1320 | val loss: 0.1836\n","epoch: 51/100 | step: 100/263 | trn loss: 0.1316 | val loss: 0.1896\n","epoch: 51/100 | step: 200/263 | trn loss: 0.1319 | val loss: 0.1782\n","epoch: 52/100 | step: 100/263 | trn loss: 0.1263 | val loss: 0.1832\n","epoch: 52/100 | step: 200/263 | trn loss: 0.1274 | val loss: 0.1789\n","epoch: 53/100 | step: 100/263 | trn loss: 0.1279 | val loss: 0.1868\n","epoch: 53/100 | step: 200/263 | trn loss: 0.1262 | val loss: 0.1851\n","epoch: 54/100 | step: 100/263 | trn loss: 0.1235 | val loss: 0.1929\n","epoch: 54/100 | step: 200/263 | trn loss: 0.1201 | val loss: 0.1774\n","epoch: 55/100 | step: 100/263 | trn loss: 0.1237 | val loss: 0.1727\n","epoch: 55/100 | step: 200/263 | trn loss: 0.1269 | val loss: 0.1779\n","epoch: 56/100 | step: 100/263 | trn loss: 0.1207 | val loss: 0.1793\n","epoch: 56/100 | step: 200/263 | trn loss: 0.1262 | val loss: 0.1746\n","epoch: 57/100 | step: 100/263 | trn loss: 0.1241 | val loss: 0.1743\n","epoch: 57/100 | step: 200/263 | trn loss: 0.1198 | val loss: 0.1731\n","epoch: 58/100 | step: 100/263 | trn loss: 0.1198 | val loss: 0.1786\n","epoch: 58/100 | step: 200/263 | trn loss: 0.1203 | val loss: 0.1763\n","epoch: 59/100 | step: 100/263 | trn loss: 0.1204 | val loss: 0.1766\n","epoch: 59/100 | step: 200/263 | trn loss: 0.1184 | val loss: 0.1721\n","epoch: 60/100 | step: 100/263 | trn loss: 0.1223 | val loss: 0.1681\n","epoch: 60/100 | step: 200/263 | trn loss: 0.1133 | val loss: 0.1742\n","epoch: 61/100 | step: 100/263 | trn loss: 0.1178 | val loss: 0.1774\n","epoch: 61/100 | step: 200/263 | trn loss: 0.1190 | val loss: 0.1624\n","epoch: 62/100 | step: 100/263 | trn loss: 0.1128 | val loss: 0.1656\n","epoch: 62/100 | step: 200/263 | trn loss: 0.1095 | val loss: 0.1721\n","epoch: 63/100 | step: 100/263 | trn loss: 0.1165 | val loss: 0.1784\n","epoch: 63/100 | step: 200/263 | trn loss: 0.1147 | val loss: 0.1720\n","epoch: 64/100 | step: 100/263 | trn loss: 0.1179 | val loss: 0.1666\n","epoch: 64/100 | step: 200/263 | trn loss: 0.1077 | val loss: 0.1614\n","epoch: 65/100 | step: 100/263 | trn loss: 0.1109 | val loss: 0.1692\n","epoch: 65/100 | step: 200/263 | trn loss: 0.1107 | val loss: 0.1641\n","epoch: 66/100 | step: 100/263 | trn loss: 0.1095 | val loss: 0.1809\n","epoch: 66/100 | step: 200/263 | trn loss: 0.1108 | val loss: 0.1780\n","epoch: 67/100 | step: 100/263 | trn loss: 0.1122 | val loss: 0.1718\n","epoch: 67/100 | step: 200/263 | trn loss: 0.1135 | val loss: 0.1753\n","epoch: 68/100 | step: 100/263 | trn loss: 0.1091 | val loss: 0.1627\n","epoch: 68/100 | step: 200/263 | trn loss: 0.1069 | val loss: 0.1609\n","epoch: 69/100 | step: 100/263 | trn loss: 0.1075 | val loss: 0.1593\n","epoch: 69/100 | step: 200/263 | trn loss: 0.1056 | val loss: 0.1713\n","epoch: 70/100 | step: 100/263 | trn loss: 0.1081 | val loss: 0.1667\n","epoch: 70/100 | step: 200/263 | trn loss: 0.1022 | val loss: 0.1632\n","epoch: 71/100 | step: 100/263 | trn loss: 0.0955 | val loss: 0.1674\n","epoch: 71/100 | step: 200/263 | trn loss: 0.1125 | val loss: 0.1698\n","epoch: 72/100 | step: 100/263 | trn loss: 0.0980 | val loss: 0.1651\n","epoch: 72/100 | step: 200/263 | trn loss: 0.1003 | val loss: 0.1600\n","epoch: 73/100 | step: 100/263 | trn loss: 0.0991 | val loss: 0.1613\n","epoch: 73/100 | step: 200/263 | trn loss: 0.1046 | val loss: 0.1688\n","epoch: 74/100 | step: 100/263 | trn loss: 0.0989 | val loss: 0.1636\n","epoch: 74/100 | step: 200/263 | trn loss: 0.1010 | val loss: 0.1617\n","epoch: 75/100 | step: 100/263 | trn loss: 0.0979 | val loss: 0.1674\n","epoch: 75/100 | step: 200/263 | trn loss: 0.1030 | val loss: 0.1704\n","epoch: 76/100 | step: 100/263 | trn loss: 0.0964 | val loss: 0.1587\n","epoch: 76/100 | step: 200/263 | trn loss: 0.0982 | val loss: 0.1554\n","epoch: 77/100 | step: 100/263 | trn loss: 0.1019 | val loss: 0.1644\n","epoch: 77/100 | step: 200/263 | trn loss: 0.0939 | val loss: 0.1593\n","epoch: 78/100 | step: 100/263 | trn loss: 0.1017 | val loss: 0.1630\n","epoch: 78/100 | step: 200/263 | trn loss: 0.0972 | val loss: 0.1690\n","epoch: 79/100 | step: 100/263 | trn loss: 0.0985 | val loss: 0.1628\n","epoch: 79/100 | step: 200/263 | trn loss: 0.0977 | val loss: 0.1647\n","epoch: 80/100 | step: 100/263 | trn loss: 0.0937 | val loss: 0.1587\n","epoch: 80/100 | step: 200/263 | trn loss: 0.0964 | val loss: 0.1532\n","epoch: 81/100 | step: 100/263 | trn loss: 0.0916 | val loss: 0.1582\n","epoch: 81/100 | step: 200/263 | trn loss: 0.0962 | val loss: 0.1679\n","epoch: 82/100 | step: 100/263 | trn loss: 0.0923 | val loss: 0.1553\n","epoch: 82/100 | step: 200/263 | trn loss: 0.0984 | val loss: 0.1605\n","epoch: 83/100 | step: 100/263 | trn loss: 0.0999 | val loss: 0.1530\n","epoch: 83/100 | step: 200/263 | trn loss: 0.0881 | val loss: 0.1546\n","epoch: 84/100 | step: 100/263 | trn loss: 0.0936 | val loss: 0.1599\n","epoch: 84/100 | step: 200/263 | trn loss: 0.0880 | val loss: 0.1513\n","epoch: 85/100 | step: 100/263 | trn loss: 0.0942 | val loss: 0.1523\n","epoch: 85/100 | step: 200/263 | trn loss: 0.0898 | val loss: 0.1545\n","epoch: 86/100 | step: 100/263 | trn loss: 0.0883 | val loss: 0.1550\n","epoch: 86/100 | step: 200/263 | trn loss: 0.0888 | val loss: 0.1562\n","epoch: 87/100 | step: 100/263 | trn loss: 0.0914 | val loss: 0.1575\n","epoch: 87/100 | step: 200/263 | trn loss: 0.0910 | val loss: 0.1515\n","epoch: 88/100 | step: 100/263 | trn loss: 0.0921 | val loss: 0.1464\n","epoch: 88/100 | step: 200/263 | trn loss: 0.0918 | val loss: 0.1532\n","epoch: 89/100 | step: 100/263 | trn loss: 0.0876 | val loss: 0.1626\n","epoch: 89/100 | step: 200/263 | trn loss: 0.0922 | val loss: 0.1465\n","epoch: 90/100 | step: 100/263 | trn loss: 0.0913 | val loss: 0.1542\n","epoch: 90/100 | step: 200/263 | trn loss: 0.0854 | val loss: 0.1583\n","epoch: 91/100 | step: 100/263 | trn loss: 0.0877 | val loss: 0.1560\n","epoch: 91/100 | step: 200/263 | trn loss: 0.0872 | val loss: 0.1705\n","epoch: 92/100 | step: 100/263 | trn loss: 0.0842 | val loss: 0.1473\n","epoch: 92/100 | step: 200/263 | trn loss: 0.0888 | val loss: 0.1533\n","epoch: 93/100 | step: 100/263 | trn loss: 0.0845 | val loss: 0.1562\n","epoch: 93/100 | step: 200/263 | trn loss: 0.0922 | val loss: 0.1494\n","epoch: 94/100 | step: 100/263 | trn loss: 0.0883 | val loss: 0.1511\n","epoch: 94/100 | step: 200/263 | trn loss: 0.0878 | val loss: 0.1539\n","epoch: 95/100 | step: 100/263 | trn loss: 0.0862 | val loss: 0.1594\n","epoch: 95/100 | step: 200/263 | trn loss: 0.0876 | val loss: 0.1594\n","epoch: 96/100 | step: 100/263 | trn loss: 0.0843 | val loss: 0.1554\n","epoch: 96/100 | step: 200/263 | trn loss: 0.0804 | val loss: 0.1484\n","epoch: 97/100 | step: 100/263 | trn loss: 0.0878 | val loss: 0.1564\n","epoch: 97/100 | step: 200/263 | trn loss: 0.0807 | val loss: 0.1540\n","epoch: 98/100 | step: 100/263 | trn loss: 0.0845 | val loss: 0.1487\n","epoch: 98/100 | step: 200/263 | trn loss: 0.0818 | val loss: 0.1536\n","epoch: 99/100 | step: 100/263 | trn loss: 0.0831 | val loss: 0.1551\n","epoch: 99/100 | step: 200/263 | trn loss: 0.0789 | val loss: 0.1568\n","epoch: 100/100 | step: 100/263 | trn loss: 0.0811 | val loss: 0.1576\n","epoch: 100/100 | step: 200/263 | trn loss: 0.0824 | val loss: 0.1462\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nxRHuslUv1XO","colab_type":"text"},"source":["3층, weight initialization, batch norm, dropout 셋 다 쓰는건 성능 so bad "]},{"cell_type":"code","metadata":{"id":"se3gAq_Hv-ig","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9051550e-b648-4e34-ce39-856602f12886","executionInfo":{"status":"ok","timestamp":1584453304053,"user_tz":-540,"elapsed":293164,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["# 4 layers, weight initialization(he), dropout(0.5)\n","trn_loss_list = []\n","val_loss_list = []\n","num_epochs = 100\n","num_batches = len(traindataloader)\n","for epoch in range(num_epochs):  # loop over the dataset multiple times\n","\n","    trn_loss = 0.0\n","    for i, data in enumerate(traindataloader):\n","        inputs, labels = data\n","        inputs = torch.tensor(inputs, device=device).float()\n","        labels = torch.tensor(labels, device=device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # print(type(inputs), type(labels), type(outputs))\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        # print(type(inputs), type(labels), type(outputs))\n","        loss = criterion(outputs, labels)\n","        # back propagation\n","        loss.backward()\n","        # weight update\n","        optimizer.step()\n","        # print statistics\n","        trn_loss += loss.item()\n","\n","        # del(memory issue)\n","        del loss\n","        del outputs\n","\n","        # 학습과정 출력\n","        if (i+1) % 100 == 0: # every 100 mini-batches\n","            with torch.no_grad(): # very very very very important!!!\n","                val_loss = 0.0\n","                for j, val in enumerate(validdataloader):\n","                    val_x, val_label = val\n","                    val_x = torch.tensor(val_x, device=device).float()\n","                    val_label = torch.tensor(val_label, device=device)\n","                    val_output = net(val_x)\n","                    v_loss = criterion(val_output, val_label)\n","                    val_loss += v_loss\n","                       \n","            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n","                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(validdataloader)\n","            ))            \n","            \n","            trn_loss_list.append(trn_loss/100)\n","            val_loss_list.append(val_loss/len(validdataloader))\n","            trn_loss = 0.0        \n","\n","       \n","\n","    #print(\"Epoch : {} loss: {}\".format(epoch, running_loss))\n","print('Finished Training')"],"execution_count":84,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 1/100 | step: 100/263 | trn loss: 2.2025 | val loss: 1.9593\n","epoch: 1/100 | step: 200/263 | trn loss: 1.7052 | val loss: 1.4076\n","epoch: 2/100 | step: 100/263 | trn loss: 0.9714 | val loss: 0.8318\n","epoch: 2/100 | step: 200/263 | trn loss: 0.7378 | val loss: 0.6511\n","epoch: 3/100 | step: 100/263 | trn loss: 0.5450 | val loss: 0.4995\n","epoch: 3/100 | step: 200/263 | trn loss: 0.4802 | val loss: 0.4459\n","epoch: 4/100 | step: 100/263 | trn loss: 0.3976 | val loss: 0.3801\n","epoch: 4/100 | step: 200/263 | trn loss: 0.3783 | val loss: 0.3560\n","epoch: 5/100 | step: 100/263 | trn loss: 0.3339 | val loss: 0.3242\n","epoch: 5/100 | step: 200/263 | trn loss: 0.3133 | val loss: 0.3112\n","epoch: 6/100 | step: 100/263 | trn loss: 0.2931 | val loss: 0.2814\n","epoch: 6/100 | step: 200/263 | trn loss: 0.2716 | val loss: 0.2614\n","epoch: 7/100 | step: 100/263 | trn loss: 0.2563 | val loss: 0.2530\n","epoch: 7/100 | step: 200/263 | trn loss: 0.2385 | val loss: 0.2557\n","epoch: 8/100 | step: 100/263 | trn loss: 0.2337 | val loss: 0.2379\n","epoch: 8/100 | step: 200/263 | trn loss: 0.2301 | val loss: 0.2304\n","epoch: 9/100 | step: 100/263 | trn loss: 0.2140 | val loss: 0.2222\n","epoch: 9/100 | step: 200/263 | trn loss: 0.2041 | val loss: 0.2153\n","epoch: 10/100 | step: 100/263 | trn loss: 0.1946 | val loss: 0.2113\n","epoch: 10/100 | step: 200/263 | trn loss: 0.1912 | val loss: 0.1954\n","epoch: 11/100 | step: 100/263 | trn loss: 0.1784 | val loss: 0.1939\n","epoch: 11/100 | step: 200/263 | trn loss: 0.1859 | val loss: 0.1970\n","epoch: 12/100 | step: 100/263 | trn loss: 0.1790 | val loss: 0.1828\n","epoch: 12/100 | step: 200/263 | trn loss: 0.1678 | val loss: 0.1827\n","epoch: 13/100 | step: 100/263 | trn loss: 0.1710 | val loss: 0.1844\n","epoch: 13/100 | step: 200/263 | trn loss: 0.1618 | val loss: 0.1736\n","epoch: 14/100 | step: 100/263 | trn loss: 0.1607 | val loss: 0.1780\n","epoch: 14/100 | step: 200/263 | trn loss: 0.1509 | val loss: 0.1717\n","epoch: 15/100 | step: 100/263 | trn loss: 0.1514 | val loss: 0.1629\n","epoch: 15/100 | step: 200/263 | trn loss: 0.1455 | val loss: 0.1607\n","epoch: 16/100 | step: 100/263 | trn loss: 0.1494 | val loss: 0.1623\n","epoch: 16/100 | step: 200/263 | trn loss: 0.1382 | val loss: 0.1545\n","epoch: 17/100 | step: 100/263 | trn loss: 0.1426 | val loss: 0.1538\n","epoch: 17/100 | step: 200/263 | trn loss: 0.1351 | val loss: 0.1528\n","epoch: 18/100 | step: 100/263 | trn loss: 0.1375 | val loss: 0.1481\n","epoch: 18/100 | step: 200/263 | trn loss: 0.1352 | val loss: 0.1486\n","epoch: 19/100 | step: 100/263 | trn loss: 0.1285 | val loss: 0.1475\n","epoch: 19/100 | step: 200/263 | trn loss: 0.1227 | val loss: 0.1451\n","epoch: 20/100 | step: 100/263 | trn loss: 0.1198 | val loss: 0.1453\n","epoch: 20/100 | step: 200/263 | trn loss: 0.1209 | val loss: 0.1365\n","epoch: 21/100 | step: 100/263 | trn loss: 0.1205 | val loss: 0.1364\n","epoch: 21/100 | step: 200/263 | trn loss: 0.1171 | val loss: 0.1416\n","epoch: 22/100 | step: 100/263 | trn loss: 0.1104 | val loss: 0.1348\n","epoch: 22/100 | step: 200/263 | trn loss: 0.1162 | val loss: 0.1374\n","epoch: 23/100 | step: 100/263 | trn loss: 0.1123 | val loss: 0.1316\n","epoch: 23/100 | step: 200/263 | trn loss: 0.1043 | val loss: 0.1300\n","epoch: 24/100 | step: 100/263 | trn loss: 0.1059 | val loss: 0.1357\n","epoch: 24/100 | step: 200/263 | trn loss: 0.1089 | val loss: 0.1321\n","epoch: 25/100 | step: 100/263 | trn loss: 0.1071 | val loss: 0.1279\n","epoch: 25/100 | step: 200/263 | trn loss: 0.1031 | val loss: 0.1326\n","epoch: 26/100 | step: 100/263 | trn loss: 0.0968 | val loss: 0.1248\n","epoch: 26/100 | step: 200/263 | trn loss: 0.0987 | val loss: 0.1260\n","epoch: 27/100 | step: 100/263 | trn loss: 0.1009 | val loss: 0.1208\n","epoch: 27/100 | step: 200/263 | trn loss: 0.0929 | val loss: 0.1213\n","epoch: 28/100 | step: 100/263 | trn loss: 0.0965 | val loss: 0.1151\n","epoch: 28/100 | step: 200/263 | trn loss: 0.0953 | val loss: 0.1208\n","epoch: 29/100 | step: 100/263 | trn loss: 0.0929 | val loss: 0.1153\n","epoch: 29/100 | step: 200/263 | trn loss: 0.0935 | val loss: 0.1192\n","epoch: 30/100 | step: 100/263 | trn loss: 0.0831 | val loss: 0.1171\n","epoch: 30/100 | step: 200/263 | trn loss: 0.0916 | val loss: 0.1222\n","epoch: 31/100 | step: 100/263 | trn loss: 0.0858 | val loss: 0.1170\n","epoch: 31/100 | step: 200/263 | trn loss: 0.0878 | val loss: 0.1179\n","epoch: 32/100 | step: 100/263 | trn loss: 0.0822 | val loss: 0.1131\n","epoch: 32/100 | step: 200/263 | trn loss: 0.0834 | val loss: 0.1194\n","epoch: 33/100 | step: 100/263 | trn loss: 0.0833 | val loss: 0.1123\n","epoch: 33/100 | step: 200/263 | trn loss: 0.0859 | val loss: 0.1171\n","epoch: 34/100 | step: 100/263 | trn loss: 0.0786 | val loss: 0.1128\n","epoch: 34/100 | step: 200/263 | trn loss: 0.0818 | val loss: 0.1118\n","epoch: 35/100 | step: 100/263 | trn loss: 0.0741 | val loss: 0.1014\n","epoch: 35/100 | step: 200/263 | trn loss: 0.0773 | val loss: 0.1112\n","epoch: 36/100 | step: 100/263 | trn loss: 0.0723 | val loss: 0.1088\n","epoch: 36/100 | step: 200/263 | trn loss: 0.0742 | val loss: 0.1067\n","epoch: 37/100 | step: 100/263 | trn loss: 0.0677 | val loss: 0.1119\n","epoch: 37/100 | step: 200/263 | trn loss: 0.0749 | val loss: 0.1124\n","epoch: 38/100 | step: 100/263 | trn loss: 0.0703 | val loss: 0.1047\n","epoch: 38/100 | step: 200/263 | trn loss: 0.0752 | val loss: 0.1087\n","epoch: 39/100 | step: 100/263 | trn loss: 0.0678 | val loss: 0.1077\n","epoch: 39/100 | step: 200/263 | trn loss: 0.0711 | val loss: 0.1075\n","epoch: 40/100 | step: 100/263 | trn loss: 0.0662 | val loss: 0.1070\n","epoch: 40/100 | step: 200/263 | trn loss: 0.0724 | val loss: 0.1038\n","epoch: 41/100 | step: 100/263 | trn loss: 0.0647 | val loss: 0.1028\n","epoch: 41/100 | step: 200/263 | trn loss: 0.0639 | val loss: 0.1033\n","epoch: 42/100 | step: 100/263 | trn loss: 0.0687 | val loss: 0.1038\n","epoch: 42/100 | step: 200/263 | trn loss: 0.0674 | val loss: 0.1022\n","epoch: 43/100 | step: 100/263 | trn loss: 0.0629 | val loss: 0.0993\n","epoch: 43/100 | step: 200/263 | trn loss: 0.0631 | val loss: 0.0969\n","epoch: 44/100 | step: 100/263 | trn loss: 0.0666 | val loss: 0.1022\n","epoch: 44/100 | step: 200/263 | trn loss: 0.0655 | val loss: 0.0982\n","epoch: 45/100 | step: 100/263 | trn loss: 0.0627 | val loss: 0.1011\n","epoch: 45/100 | step: 200/263 | trn loss: 0.0632 | val loss: 0.1021\n","epoch: 46/100 | step: 100/263 | trn loss: 0.0621 | val loss: 0.1029\n","epoch: 46/100 | step: 200/263 | trn loss: 0.0592 | val loss: 0.1016\n","epoch: 47/100 | step: 100/263 | trn loss: 0.0571 | val loss: 0.1011\n","epoch: 47/100 | step: 200/263 | trn loss: 0.0591 | val loss: 0.1043\n","epoch: 48/100 | step: 100/263 | trn loss: 0.0525 | val loss: 0.0968\n","epoch: 48/100 | step: 200/263 | trn loss: 0.0660 | val loss: 0.0962\n","epoch: 49/100 | step: 100/263 | trn loss: 0.0588 | val loss: 0.1033\n","epoch: 49/100 | step: 200/263 | trn loss: 0.0581 | val loss: 0.1010\n","epoch: 50/100 | step: 100/263 | trn loss: 0.0594 | val loss: 0.0994\n","epoch: 50/100 | step: 200/263 | trn loss: 0.0563 | val loss: 0.0955\n","epoch: 51/100 | step: 100/263 | trn loss: 0.0553 | val loss: 0.0958\n","epoch: 51/100 | step: 200/263 | trn loss: 0.0585 | val loss: 0.0923\n","epoch: 52/100 | step: 100/263 | trn loss: 0.0557 | val loss: 0.0923\n","epoch: 52/100 | step: 200/263 | trn loss: 0.0581 | val loss: 0.0981\n","epoch: 53/100 | step: 100/263 | trn loss: 0.0522 | val loss: 0.0954\n","epoch: 53/100 | step: 200/263 | trn loss: 0.0553 | val loss: 0.0930\n","epoch: 54/100 | step: 100/263 | trn loss: 0.0514 | val loss: 0.0942\n","epoch: 54/100 | step: 200/263 | trn loss: 0.0545 | val loss: 0.0943\n","epoch: 55/100 | step: 100/263 | trn loss: 0.0533 | val loss: 0.0949\n","epoch: 55/100 | step: 200/263 | trn loss: 0.0541 | val loss: 0.0884\n","epoch: 56/100 | step: 100/263 | trn loss: 0.0517 | val loss: 0.0956\n","epoch: 56/100 | step: 200/263 | trn loss: 0.0567 | val loss: 0.0943\n","epoch: 57/100 | step: 100/263 | trn loss: 0.0526 | val loss: 0.0845\n","epoch: 57/100 | step: 200/263 | trn loss: 0.0500 | val loss: 0.0971\n","epoch: 58/100 | step: 100/263 | trn loss: 0.0524 | val loss: 0.0912\n","epoch: 58/100 | step: 200/263 | trn loss: 0.0508 | val loss: 0.0928\n","epoch: 59/100 | step: 100/263 | trn loss: 0.0513 | val loss: 0.0862\n","epoch: 59/100 | step: 200/263 | trn loss: 0.0477 | val loss: 0.0896\n","epoch: 60/100 | step: 100/263 | trn loss: 0.0494 | val loss: 0.0919\n","epoch: 60/100 | step: 200/263 | trn loss: 0.0501 | val loss: 0.0879\n","epoch: 61/100 | step: 100/263 | trn loss: 0.0499 | val loss: 0.0916\n","epoch: 61/100 | step: 200/263 | trn loss: 0.0494 | val loss: 0.0907\n","epoch: 62/100 | step: 100/263 | trn loss: 0.0478 | val loss: 0.0940\n","epoch: 62/100 | step: 200/263 | trn loss: 0.0489 | val loss: 0.0929\n","epoch: 63/100 | step: 100/263 | trn loss: 0.0466 | val loss: 0.0945\n","epoch: 63/100 | step: 200/263 | trn loss: 0.0453 | val loss: 0.0898\n","epoch: 64/100 | step: 100/263 | trn loss: 0.0464 | val loss: 0.0868\n","epoch: 64/100 | step: 200/263 | trn loss: 0.0484 | val loss: 0.0873\n","epoch: 65/100 | step: 100/263 | trn loss: 0.0471 | val loss: 0.0883\n","epoch: 65/100 | step: 200/263 | trn loss: 0.0470 | val loss: 0.0916\n","epoch: 66/100 | step: 100/263 | trn loss: 0.0434 | val loss: 0.0922\n","epoch: 66/100 | step: 200/263 | trn loss: 0.0441 | val loss: 0.0855\n","epoch: 67/100 | step: 100/263 | trn loss: 0.0418 | val loss: 0.0811\n","epoch: 67/100 | step: 200/263 | trn loss: 0.0478 | val loss: 0.0890\n","epoch: 68/100 | step: 100/263 | trn loss: 0.0452 | val loss: 0.0851\n","epoch: 68/100 | step: 200/263 | trn loss: 0.0452 | val loss: 0.0879\n","epoch: 69/100 | step: 100/263 | trn loss: 0.0405 | val loss: 0.0859\n","epoch: 69/100 | step: 200/263 | trn loss: 0.0475 | val loss: 0.0885\n","epoch: 70/100 | step: 100/263 | trn loss: 0.0426 | val loss: 0.0839\n","epoch: 70/100 | step: 200/263 | trn loss: 0.0430 | val loss: 0.0917\n","epoch: 71/100 | step: 100/263 | trn loss: 0.0427 | val loss: 0.0862\n","epoch: 71/100 | step: 200/263 | trn loss: 0.0452 | val loss: 0.0869\n","epoch: 72/100 | step: 100/263 | trn loss: 0.0428 | val loss: 0.0908\n","epoch: 72/100 | step: 200/263 | trn loss: 0.0410 | val loss: 0.0930\n","epoch: 73/100 | step: 100/263 | trn loss: 0.0392 | val loss: 0.0898\n","epoch: 73/100 | step: 200/263 | trn loss: 0.0450 | val loss: 0.0885\n","epoch: 74/100 | step: 100/263 | trn loss: 0.0448 | val loss: 0.0901\n","epoch: 74/100 | step: 200/263 | trn loss: 0.0402 | val loss: 0.0810\n","epoch: 75/100 | step: 100/263 | trn loss: 0.0407 | val loss: 0.0903\n","epoch: 75/100 | step: 200/263 | trn loss: 0.0437 | val loss: 0.0833\n","epoch: 76/100 | step: 100/263 | trn loss: 0.0384 | val loss: 0.0884\n","epoch: 76/100 | step: 200/263 | trn loss: 0.0413 | val loss: 0.0819\n","epoch: 77/100 | step: 100/263 | trn loss: 0.0428 | val loss: 0.0807\n","epoch: 77/100 | step: 200/263 | trn loss: 0.0414 | val loss: 0.0852\n","epoch: 78/100 | step: 100/263 | trn loss: 0.0412 | val loss: 0.0858\n","epoch: 78/100 | step: 200/263 | trn loss: 0.0414 | val loss: 0.0854\n","epoch: 79/100 | step: 100/263 | trn loss: 0.0387 | val loss: 0.0821\n","epoch: 79/100 | step: 200/263 | trn loss: 0.0388 | val loss: 0.0852\n","epoch: 80/100 | step: 100/263 | trn loss: 0.0429 | val loss: 0.0876\n","epoch: 80/100 | step: 200/263 | trn loss: 0.0411 | val loss: 0.0824\n","epoch: 81/100 | step: 100/263 | trn loss: 0.0390 | val loss: 0.0864\n","epoch: 81/100 | step: 200/263 | trn loss: 0.0418 | val loss: 0.0859\n","epoch: 82/100 | step: 100/263 | trn loss: 0.0413 | val loss: 0.0851\n","epoch: 82/100 | step: 200/263 | trn loss: 0.0426 | val loss: 0.0852\n","epoch: 83/100 | step: 100/263 | trn loss: 0.0410 | val loss: 0.0855\n","epoch: 83/100 | step: 200/263 | trn loss: 0.0405 | val loss: 0.0847\n","epoch: 84/100 | step: 100/263 | trn loss: 0.0371 | val loss: 0.0852\n","epoch: 84/100 | step: 200/263 | trn loss: 0.0424 | val loss: 0.0852\n","epoch: 85/100 | step: 100/263 | trn loss: 0.0390 | val loss: 0.0849\n","epoch: 85/100 | step: 200/263 | trn loss: 0.0408 | val loss: 0.0823\n","epoch: 86/100 | step: 100/263 | trn loss: 0.0371 | val loss: 0.0835\n","epoch: 86/100 | step: 200/263 | trn loss: 0.0375 | val loss: 0.0874\n","epoch: 87/100 | step: 100/263 | trn loss: 0.0374 | val loss: 0.0772\n","epoch: 87/100 | step: 200/263 | trn loss: 0.0400 | val loss: 0.0821\n","epoch: 88/100 | step: 100/263 | trn loss: 0.0382 | val loss: 0.0790\n","epoch: 88/100 | step: 200/263 | trn loss: 0.0373 | val loss: 0.0809\n","epoch: 89/100 | step: 100/263 | trn loss: 0.0381 | val loss: 0.0779\n","epoch: 89/100 | step: 200/263 | trn loss: 0.0359 | val loss: 0.0797\n","epoch: 90/100 | step: 100/263 | trn loss: 0.0409 | val loss: 0.0814\n","epoch: 90/100 | step: 200/263 | trn loss: 0.0370 | val loss: 0.0853\n","epoch: 91/100 | step: 100/263 | trn loss: 0.0346 | val loss: 0.0836\n","epoch: 91/100 | step: 200/263 | trn loss: 0.0380 | val loss: 0.0799\n","epoch: 92/100 | step: 100/263 | trn loss: 0.0388 | val loss: 0.0793\n","epoch: 92/100 | step: 200/263 | trn loss: 0.0365 | val loss: 0.0858\n","epoch: 93/100 | step: 100/263 | trn loss: 0.0369 | val loss: 0.0817\n","epoch: 93/100 | step: 200/263 | trn loss: 0.0373 | val loss: 0.0841\n","epoch: 94/100 | step: 100/263 | trn loss: 0.0350 | val loss: 0.0797\n","epoch: 94/100 | step: 200/263 | trn loss: 0.0371 | val loss: 0.0852\n","epoch: 95/100 | step: 100/263 | trn loss: 0.0362 | val loss: 0.0802\n","epoch: 95/100 | step: 200/263 | trn loss: 0.0368 | val loss: 0.0810\n","epoch: 96/100 | step: 100/263 | trn loss: 0.0327 | val loss: 0.0857\n","epoch: 96/100 | step: 200/263 | trn loss: 0.0383 | val loss: 0.0765\n","epoch: 97/100 | step: 100/263 | trn loss: 0.0364 | val loss: 0.0791\n","epoch: 97/100 | step: 200/263 | trn loss: 0.0356 | val loss: 0.0789\n","epoch: 98/100 | step: 100/263 | trn loss: 0.0358 | val loss: 0.0775\n","epoch: 98/100 | step: 200/263 | trn loss: 0.0365 | val loss: 0.0817\n","epoch: 99/100 | step: 100/263 | trn loss: 0.0334 | val loss: 0.0823\n","epoch: 99/100 | step: 200/263 | trn loss: 0.0363 | val loss: 0.0746\n","epoch: 100/100 | step: 100/263 | trn loss: 0.0363 | val loss: 0.0797\n","epoch: 100/100 | step: 200/263 | trn loss: 0.0322 | val loss: 0.0808\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5ieUYvyKwr4n","colab_type":"text"},"source":["4층, weight initialization, dropout만 쓴 얘도 별로... "]},{"cell_type":"code","metadata":{"id":"pbvupT6Bx4JK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9bcf4dc9-acad-40a4-805d-fdb3db82a072","executionInfo":{"status":"ok","timestamp":1584453829669,"user_tz":-540,"elapsed":338116,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["# 4 layers, weight initialization(he), batch normalization\n","trn_loss_list = []\n","val_loss_list = []\n","num_epochs = 100\n","num_batches = len(traindataloader)\n","for epoch in range(num_epochs):  # loop over the dataset multiple times\n","\n","    trn_loss = 0.0\n","    for i, data in enumerate(traindataloader):\n","        inputs, labels = data\n","        inputs = torch.tensor(inputs, device=device).float()\n","        labels = torch.tensor(labels, device=device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # print(type(inputs), type(labels), type(outputs))\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        # print(type(inputs), type(labels), type(outputs))\n","        loss = criterion(outputs, labels)\n","        # back propagation\n","        loss.backward()\n","        # weight update\n","        optimizer.step()\n","        # print statistics\n","        trn_loss += loss.item()\n","\n","        # del(memory issue)\n","        del loss\n","        del outputs\n","\n","        # 학습과정 출력\n","        if (i+1) % 100 == 0: # every 100 mini-batches\n","            with torch.no_grad(): # very very very very important!!!\n","                val_loss = 0.0\n","                for j, val in enumerate(validdataloader):\n","                    val_x, val_label = val\n","                    val_x = torch.tensor(val_x, device=device).float()\n","                    val_label = torch.tensor(val_label, device=device)\n","                    val_output = net(val_x)\n","                    v_loss = criterion(val_output, val_label)\n","                    val_loss += v_loss\n","                       \n","            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n","                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(validdataloader)\n","            ))            \n","            \n","            trn_loss_list.append(trn_loss/100)\n","            val_loss_list.append(val_loss/len(validdataloader))\n","            trn_loss = 0.0        \n","\n","       \n","\n","    #print(\"Epoch : {} loss: {}\".format(epoch, running_loss))\n","print('Finished Training')"],"execution_count":92,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 1/100 | step: 100/263 | trn loss: 1.1602 | val loss: 0.5769\n","epoch: 1/100 | step: 200/263 | trn loss: 0.4292 | val loss: 0.3414\n","epoch: 2/100 | step: 100/263 | trn loss: 0.2189 | val loss: 0.2274\n","epoch: 2/100 | step: 200/263 | trn loss: 0.1855 | val loss: 0.1915\n","epoch: 3/100 | step: 100/263 | trn loss: 0.1263 | val loss: 0.1569\n","epoch: 3/100 | step: 200/263 | trn loss: 0.1238 | val loss: 0.1444\n","epoch: 4/100 | step: 100/263 | trn loss: 0.0917 | val loss: 0.1276\n","epoch: 4/100 | step: 200/263 | trn loss: 0.0905 | val loss: 0.1197\n","epoch: 5/100 | step: 100/263 | trn loss: 0.0690 | val loss: 0.1069\n","epoch: 5/100 | step: 200/263 | trn loss: 0.0690 | val loss: 0.1011\n","epoch: 6/100 | step: 100/263 | trn loss: 0.0499 | val loss: 0.0986\n","epoch: 6/100 | step: 200/263 | trn loss: 0.0558 | val loss: 0.0929\n","epoch: 7/100 | step: 100/263 | trn loss: 0.0436 | val loss: 0.0906\n","epoch: 7/100 | step: 200/263 | trn loss: 0.0396 | val loss: 0.0847\n","epoch: 8/100 | step: 100/263 | trn loss: 0.0337 | val loss: 0.0825\n","epoch: 8/100 | step: 200/263 | trn loss: 0.0314 | val loss: 0.0814\n","epoch: 9/100 | step: 100/263 | trn loss: 0.0246 | val loss: 0.0802\n","epoch: 9/100 | step: 200/263 | trn loss: 0.0261 | val loss: 0.0765\n","epoch: 10/100 | step: 100/263 | trn loss: 0.0192 | val loss: 0.0756\n","epoch: 10/100 | step: 200/263 | trn loss: 0.0210 | val loss: 0.0758\n","epoch: 11/100 | step: 100/263 | trn loss: 0.0158 | val loss: 0.0729\n","epoch: 11/100 | step: 200/263 | trn loss: 0.0170 | val loss: 0.0705\n","epoch: 12/100 | step: 100/263 | trn loss: 0.0143 | val loss: 0.0699\n","epoch: 12/100 | step: 200/263 | trn loss: 0.0127 | val loss: 0.0669\n","epoch: 13/100 | step: 100/263 | trn loss: 0.0122 | val loss: 0.0667\n","epoch: 13/100 | step: 200/263 | trn loss: 0.0109 | val loss: 0.0716\n","epoch: 14/100 | step: 100/263 | trn loss: 0.0087 | val loss: 0.0670\n","epoch: 14/100 | step: 200/263 | trn loss: 0.0103 | val loss: 0.0679\n","epoch: 15/100 | step: 100/263 | trn loss: 0.0084 | val loss: 0.0690\n","epoch: 15/100 | step: 200/263 | trn loss: 0.0086 | val loss: 0.0668\n","epoch: 16/100 | step: 100/263 | trn loss: 0.0082 | val loss: 0.0682\n","epoch: 16/100 | step: 200/263 | trn loss: 0.0081 | val loss: 0.0613\n","epoch: 17/100 | step: 100/263 | trn loss: 0.0068 | val loss: 0.0653\n","epoch: 17/100 | step: 200/263 | trn loss: 0.0074 | val loss: 0.0673\n","epoch: 18/100 | step: 100/263 | trn loss: 0.0073 | val loss: 0.0666\n","epoch: 18/100 | step: 200/263 | trn loss: 0.0070 | val loss: 0.0682\n","epoch: 19/100 | step: 100/263 | trn loss: 0.0058 | val loss: 0.0626\n","epoch: 19/100 | step: 200/263 | trn loss: 0.0061 | val loss: 0.0640\n","epoch: 20/100 | step: 100/263 | trn loss: 0.0059 | val loss: 0.0620\n","epoch: 20/100 | step: 200/263 | trn loss: 0.0060 | val loss: 0.0635\n","epoch: 21/100 | step: 100/263 | trn loss: 0.0070 | val loss: 0.0623\n","epoch: 21/100 | step: 200/263 | trn loss: 0.0066 | val loss: 0.0645\n","epoch: 22/100 | step: 100/263 | trn loss: 0.0055 | val loss: 0.0612\n","epoch: 22/100 | step: 200/263 | trn loss: 0.0053 | val loss: 0.0600\n","epoch: 23/100 | step: 100/263 | trn loss: 0.0052 | val loss: 0.0593\n","epoch: 23/100 | step: 200/263 | trn loss: 0.0056 | val loss: 0.0677\n","epoch: 24/100 | step: 100/263 | trn loss: 0.0078 | val loss: 0.0606\n","epoch: 24/100 | step: 200/263 | trn loss: 0.0063 | val loss: 0.0621\n","epoch: 25/100 | step: 100/263 | trn loss: 0.0048 | val loss: 0.0640\n","epoch: 25/100 | step: 200/263 | trn loss: 0.0059 | val loss: 0.0594\n","epoch: 26/100 | step: 100/263 | trn loss: 0.0051 | val loss: 0.0621\n","epoch: 26/100 | step: 200/263 | trn loss: 0.0048 | val loss: 0.0624\n","epoch: 27/100 | step: 100/263 | trn loss: 0.0047 | val loss: 0.0577\n","epoch: 27/100 | step: 200/263 | trn loss: 0.0063 | val loss: 0.0633\n","epoch: 28/100 | step: 100/263 | trn loss: 0.0045 | val loss: 0.0611\n","epoch: 28/100 | step: 200/263 | trn loss: 0.0086 | val loss: 0.0765\n","epoch: 29/100 | step: 100/263 | trn loss: 0.0076 | val loss: 0.0632\n","epoch: 29/100 | step: 200/263 | trn loss: 0.0085 | val loss: 0.0697\n","epoch: 30/100 | step: 100/263 | trn loss: 0.0049 | val loss: 0.0595\n","epoch: 30/100 | step: 200/263 | trn loss: 0.0052 | val loss: 0.0627\n","epoch: 31/100 | step: 100/263 | trn loss: 0.0054 | val loss: 0.0627\n","epoch: 31/100 | step: 200/263 | trn loss: 0.0043 | val loss: 0.0629\n","epoch: 32/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0577\n","epoch: 32/100 | step: 200/263 | trn loss: 0.0040 | val loss: 0.0559\n","epoch: 33/100 | step: 100/263 | trn loss: 0.0050 | val loss: 0.0608\n","epoch: 33/100 | step: 200/263 | trn loss: 0.0073 | val loss: 0.0638\n","epoch: 34/100 | step: 100/263 | trn loss: 0.0079 | val loss: 0.0633\n","epoch: 34/100 | step: 200/263 | trn loss: 0.0060 | val loss: 0.0576\n","epoch: 35/100 | step: 100/263 | trn loss: 0.0049 | val loss: 0.0597\n","epoch: 35/100 | step: 200/263 | trn loss: 0.0048 | val loss: 0.0555\n","epoch: 36/100 | step: 100/263 | trn loss: 0.0041 | val loss: 0.0536\n","epoch: 36/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0529\n","epoch: 37/100 | step: 100/263 | trn loss: 0.0110 | val loss: 0.0707\n","epoch: 37/100 | step: 200/263 | trn loss: 0.0113 | val loss: 0.0674\n","epoch: 38/100 | step: 100/263 | trn loss: 0.0093 | val loss: 0.0629\n","epoch: 38/100 | step: 200/263 | trn loss: 0.0079 | val loss: 0.0665\n","epoch: 39/100 | step: 100/263 | trn loss: 0.0078 | val loss: 0.0676\n","epoch: 39/100 | step: 200/263 | trn loss: 0.0062 | val loss: 0.0588\n","epoch: 40/100 | step: 100/263 | trn loss: 0.0043 | val loss: 0.0521\n","epoch: 40/100 | step: 200/263 | trn loss: 0.0043 | val loss: 0.0599\n","epoch: 41/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0559\n","epoch: 41/100 | step: 200/263 | trn loss: 0.0040 | val loss: 0.0542\n","epoch: 42/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0523\n","epoch: 42/100 | step: 200/263 | trn loss: 0.0047 | val loss: 0.0545\n","epoch: 43/100 | step: 100/263 | trn loss: 0.0046 | val loss: 0.0547\n","epoch: 43/100 | step: 200/263 | trn loss: 0.0094 | val loss: 0.0751\n","epoch: 44/100 | step: 100/263 | trn loss: 0.0062 | val loss: 0.0571\n","epoch: 44/100 | step: 200/263 | trn loss: 0.0058 | val loss: 0.0574\n","epoch: 45/100 | step: 100/263 | trn loss: 0.0046 | val loss: 0.0539\n","epoch: 45/100 | step: 200/263 | trn loss: 0.0042 | val loss: 0.0555\n","epoch: 46/100 | step: 100/263 | trn loss: 0.0043 | val loss: 0.0533\n","epoch: 46/100 | step: 200/263 | trn loss: 0.0048 | val loss: 0.0514\n","epoch: 47/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0545\n","epoch: 47/100 | step: 200/263 | trn loss: 0.0047 | val loss: 0.0561\n","epoch: 48/100 | step: 100/263 | trn loss: 0.0118 | val loss: 0.0686\n","epoch: 48/100 | step: 200/263 | trn loss: 0.0110 | val loss: 0.0575\n","epoch: 49/100 | step: 100/263 | trn loss: 0.0097 | val loss: 0.0545\n","epoch: 49/100 | step: 200/263 | trn loss: 0.0060 | val loss: 0.0556\n","epoch: 50/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0539\n","epoch: 50/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0522\n","epoch: 51/100 | step: 100/263 | trn loss: 0.0041 | val loss: 0.0493\n","epoch: 51/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0517\n","epoch: 52/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0508\n","epoch: 52/100 | step: 200/263 | trn loss: 0.0060 | val loss: 0.0580\n","epoch: 53/100 | step: 100/263 | trn loss: 0.0073 | val loss: 0.0670\n","epoch: 53/100 | step: 200/263 | trn loss: 0.0068 | val loss: 0.0604\n","epoch: 54/100 | step: 100/263 | trn loss: 0.0067 | val loss: 0.0509\n","epoch: 54/100 | step: 200/263 | trn loss: 0.0046 | val loss: 0.0538\n","epoch: 55/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0531\n","epoch: 55/100 | step: 200/263 | trn loss: 0.0045 | val loss: 0.0569\n","epoch: 56/100 | step: 100/263 | trn loss: 0.0038 | val loss: 0.0524\n","epoch: 56/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0497\n","epoch: 57/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0523\n","epoch: 57/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0495\n","epoch: 58/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0536\n","epoch: 58/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0499\n","epoch: 59/100 | step: 100/263 | trn loss: 0.0043 | val loss: 0.0598\n","epoch: 59/100 | step: 200/263 | trn loss: 0.0109 | val loss: 0.0937\n","epoch: 60/100 | step: 100/263 | trn loss: 0.0214 | val loss: 0.0686\n","epoch: 60/100 | step: 200/263 | trn loss: 0.0140 | val loss: 0.0629\n","epoch: 61/100 | step: 100/263 | trn loss: 0.0059 | val loss: 0.0526\n","epoch: 61/100 | step: 200/263 | trn loss: 0.0048 | val loss: 0.0501\n","epoch: 62/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0517\n","epoch: 62/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0504\n","epoch: 63/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0498\n","epoch: 63/100 | step: 200/263 | trn loss: 0.0034 | val loss: 0.0508\n","epoch: 64/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0510\n","epoch: 64/100 | step: 200/263 | trn loss: 0.0042 | val loss: 0.0533\n","epoch: 65/100 | step: 100/263 | trn loss: 0.0085 | val loss: 0.0526\n","epoch: 65/100 | step: 200/263 | trn loss: 0.0047 | val loss: 0.0521\n","epoch: 66/100 | step: 100/263 | trn loss: 0.0048 | val loss: 0.0527\n","epoch: 66/100 | step: 200/263 | trn loss: 0.0049 | val loss: 0.0504\n","epoch: 67/100 | step: 100/263 | trn loss: 0.0055 | val loss: 0.0559\n","epoch: 67/100 | step: 200/263 | trn loss: 0.0052 | val loss: 0.0511\n","epoch: 68/100 | step: 100/263 | trn loss: 0.0051 | val loss: 0.0590\n","epoch: 68/100 | step: 200/263 | trn loss: 0.0043 | val loss: 0.0526\n","epoch: 69/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0518\n","epoch: 69/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0518\n","epoch: 70/100 | step: 100/263 | trn loss: 0.0053 | val loss: 0.0587\n","epoch: 70/100 | step: 200/263 | trn loss: 0.0053 | val loss: 0.0829\n","epoch: 71/100 | step: 100/263 | trn loss: 0.0195 | val loss: 0.0728\n","epoch: 71/100 | step: 200/263 | trn loss: 0.0166 | val loss: 0.0674\n","epoch: 72/100 | step: 100/263 | trn loss: 0.0089 | val loss: 0.0578\n","epoch: 72/100 | step: 200/263 | trn loss: 0.0052 | val loss: 0.0541\n","epoch: 73/100 | step: 100/263 | trn loss: 0.0047 | val loss: 0.0540\n","epoch: 73/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0491\n","epoch: 74/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0531\n","epoch: 74/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0554\n","epoch: 75/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0529\n","epoch: 75/100 | step: 200/263 | trn loss: 0.0040 | val loss: 0.0554\n","epoch: 76/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0546\n","epoch: 76/100 | step: 200/263 | trn loss: 0.0036 | val loss: 0.0503\n","epoch: 77/100 | step: 100/263 | trn loss: 0.0041 | val loss: 0.0476\n","epoch: 77/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0524\n","epoch: 78/100 | step: 100/263 | trn loss: 0.0044 | val loss: 0.0588\n","epoch: 78/100 | step: 200/263 | trn loss: 0.0043 | val loss: 0.0527\n","epoch: 79/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0551\n","epoch: 79/100 | step: 200/263 | trn loss: 0.0042 | val loss: 0.0593\n","epoch: 80/100 | step: 100/263 | trn loss: 0.0108 | val loss: 0.0851\n","epoch: 80/100 | step: 200/263 | trn loss: 0.0157 | val loss: 0.0734\n","epoch: 81/100 | step: 100/263 | trn loss: 0.0109 | val loss: 0.0618\n","epoch: 81/100 | step: 200/263 | trn loss: 0.0080 | val loss: 0.0571\n","epoch: 82/100 | step: 100/263 | trn loss: 0.0063 | val loss: 0.0592\n","epoch: 82/100 | step: 200/263 | trn loss: 0.0044 | val loss: 0.0572\n","epoch: 83/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0508\n","epoch: 83/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0529\n","epoch: 84/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0562\n","epoch: 84/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0542\n","epoch: 85/100 | step: 100/263 | trn loss: 0.0051 | val loss: 0.0570\n","epoch: 85/100 | step: 200/263 | trn loss: 0.0051 | val loss: 0.0559\n","epoch: 86/100 | step: 100/263 | trn loss: 0.0038 | val loss: 0.0528\n","epoch: 86/100 | step: 200/263 | trn loss: 0.0044 | val loss: 0.0539\n","epoch: 87/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0505\n","epoch: 87/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0497\n","epoch: 88/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0558\n","epoch: 88/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0535\n","epoch: 89/100 | step: 100/263 | trn loss: 0.0064 | val loss: 0.0577\n","epoch: 89/100 | step: 200/263 | trn loss: 0.0051 | val loss: 0.0568\n","epoch: 90/100 | step: 100/263 | trn loss: 0.0044 | val loss: 0.0527\n","epoch: 90/100 | step: 200/263 | trn loss: 0.0067 | val loss: 0.0696\n","epoch: 91/100 | step: 100/263 | trn loss: 0.0092 | val loss: 0.0538\n","epoch: 91/100 | step: 200/263 | trn loss: 0.0084 | val loss: 0.0620\n","epoch: 92/100 | step: 100/263 | trn loss: 0.0082 | val loss: 0.0660\n","epoch: 92/100 | step: 200/263 | trn loss: 0.0082 | val loss: 0.0670\n","epoch: 93/100 | step: 100/263 | trn loss: 0.0070 | val loss: 0.0593\n","epoch: 93/100 | step: 200/263 | trn loss: 0.0049 | val loss: 0.0590\n","epoch: 94/100 | step: 100/263 | trn loss: 0.0042 | val loss: 0.0561\n","epoch: 94/100 | step: 200/263 | trn loss: 0.0042 | val loss: 0.0604\n","epoch: 95/100 | step: 100/263 | trn loss: 0.0059 | val loss: 0.0568\n","epoch: 95/100 | step: 200/263 | trn loss: 0.0051 | val loss: 0.0567\n","epoch: 96/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0578\n","epoch: 96/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0515\n","epoch: 97/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0556\n","epoch: 97/100 | step: 200/263 | trn loss: 0.0036 | val loss: 0.0514\n","epoch: 98/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0548\n","epoch: 98/100 | step: 200/263 | trn loss: 0.0040 | val loss: 0.0520\n","epoch: 99/100 | step: 100/263 | trn loss: 0.0035 | val loss: 0.0534\n","epoch: 99/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0546\n","epoch: 100/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0525\n","epoch: 100/100 | step: 200/263 | trn loss: 0.0040 | val loss: 0.0527\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kelJtTYwzw9N","colab_type":"text"},"source":["dropout 대신 batchnorm만 적용했더니 val loss 0.05로 올랐다"]},{"cell_type":"code","metadata":{"id":"Jn3FQ61D0H-X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"91bab6f9-c3e8-4f91-ecff-079e34b7b275","executionInfo":{"status":"ok","timestamp":1584455462877,"user_tz":-540,"elapsed":348762,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["# 4 layers, batch normalization\n","trn_loss_list = []\n","val_loss_list = []\n","num_epochs = 100\n","num_batches = len(traindataloader)\n","for epoch in range(num_epochs):  # loop over the dataset multiple times\n","\n","    trn_loss = 0.0\n","    for i, data in enumerate(traindataloader):\n","        inputs, labels = data\n","        inputs = torch.tensor(inputs, device=device).float()\n","        labels = torch.tensor(labels, device=device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # print(type(inputs), type(labels), type(outputs))\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        # print(type(inputs), type(labels), type(outputs))\n","        loss = criterion(outputs, labels)\n","        # back propagation\n","        loss.backward()\n","        # weight update\n","        optimizer.step()\n","        # print statistics\n","        trn_loss += loss.item()\n","\n","        # del(memory issue)\n","        del loss\n","        del outputs\n","\n","        # 학습과정 출력\n","        if (i+1) % 100 == 0: # every 100 mini-batches\n","            with torch.no_grad(): # very very very very important!!!\n","                val_loss = 0.0\n","                for j, val in enumerate(validdataloader):\n","                    val_x, val_label = val\n","                    val_x = torch.tensor(val_x, device=device).float()\n","                    val_label = torch.tensor(val_label, device=device)\n","                    val_output = net(val_x)\n","                    v_loss = criterion(val_output, val_label)\n","                    val_loss += v_loss\n","                       \n","            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n","                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(validdataloader)\n","            ))            \n","            \n","            trn_loss_list.append(trn_loss/100)\n","            val_loss_list.append(val_loss/len(validdataloader))\n","            trn_loss = 0.0        \n","\n","       \n","\n","    #print(\"Epoch : {} loss: {}\".format(epoch, running_loss))\n","print('Finished Training')"],"execution_count":116,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 1/100 | step: 100/263 | trn loss: 1.0334 | val loss: 0.5659\n","epoch: 1/100 | step: 200/263 | trn loss: 0.4274 | val loss: 0.3389\n","epoch: 2/100 | step: 100/263 | trn loss: 0.2208 | val loss: 0.2136\n","epoch: 2/100 | step: 200/263 | trn loss: 0.1674 | val loss: 0.1776\n","epoch: 3/100 | step: 100/263 | trn loss: 0.1196 | val loss: 0.1377\n","epoch: 3/100 | step: 200/263 | trn loss: 0.1014 | val loss: 0.1211\n","epoch: 4/100 | step: 100/263 | trn loss: 0.0756 | val loss: 0.1062\n","epoch: 4/100 | step: 200/263 | trn loss: 0.0689 | val loss: 0.1028\n","epoch: 5/100 | step: 100/263 | trn loss: 0.0505 | val loss: 0.0872\n","epoch: 5/100 | step: 200/263 | trn loss: 0.0475 | val loss: 0.0855\n","epoch: 6/100 | step: 100/263 | trn loss: 0.0331 | val loss: 0.0755\n","epoch: 6/100 | step: 200/263 | trn loss: 0.0331 | val loss: 0.0781\n","epoch: 7/100 | step: 100/263 | trn loss: 0.0250 | val loss: 0.0751\n","epoch: 7/100 | step: 200/263 | trn loss: 0.0246 | val loss: 0.0711\n","epoch: 8/100 | step: 100/263 | trn loss: 0.0174 | val loss: 0.0680\n","epoch: 8/100 | step: 200/263 | trn loss: 0.0179 | val loss: 0.0695\n","epoch: 9/100 | step: 100/263 | trn loss: 0.0138 | val loss: 0.0632\n","epoch: 9/100 | step: 200/263 | trn loss: 0.0131 | val loss: 0.0601\n","epoch: 10/100 | step: 100/263 | trn loss: 0.0123 | val loss: 0.0665\n","epoch: 10/100 | step: 200/263 | trn loss: 0.0113 | val loss: 0.0644\n","epoch: 11/100 | step: 100/263 | trn loss: 0.0097 | val loss: 0.0589\n","epoch: 11/100 | step: 200/263 | trn loss: 0.0098 | val loss: 0.0580\n","epoch: 12/100 | step: 100/263 | trn loss: 0.0098 | val loss: 0.0581\n","epoch: 12/100 | step: 200/263 | trn loss: 0.0078 | val loss: 0.0566\n","epoch: 13/100 | step: 100/263 | trn loss: 0.0080 | val loss: 0.0581\n","epoch: 13/100 | step: 200/263 | trn loss: 0.0075 | val loss: 0.0566\n","epoch: 14/100 | step: 100/263 | trn loss: 0.0098 | val loss: 0.0574\n","epoch: 14/100 | step: 200/263 | trn loss: 0.0070 | val loss: 0.0563\n","epoch: 15/100 | step: 100/263 | trn loss: 0.0081 | val loss: 0.0579\n","epoch: 15/100 | step: 200/263 | trn loss: 0.0090 | val loss: 0.0665\n","epoch: 16/100 | step: 100/263 | trn loss: 0.0067 | val loss: 0.0583\n","epoch: 16/100 | step: 200/263 | trn loss: 0.0058 | val loss: 0.0561\n","epoch: 17/100 | step: 100/263 | trn loss: 0.0052 | val loss: 0.0553\n","epoch: 17/100 | step: 200/263 | trn loss: 0.0048 | val loss: 0.0543\n","epoch: 18/100 | step: 100/263 | trn loss: 0.0050 | val loss: 0.0572\n","epoch: 18/100 | step: 200/263 | trn loss: 0.0064 | val loss: 0.0595\n","epoch: 19/100 | step: 100/263 | trn loss: 0.0067 | val loss: 0.0628\n","epoch: 19/100 | step: 200/263 | trn loss: 0.0092 | val loss: 0.0716\n","epoch: 20/100 | step: 100/263 | trn loss: 0.0100 | val loss: 0.0587\n","epoch: 20/100 | step: 200/263 | trn loss: 0.0089 | val loss: 0.0608\n","epoch: 21/100 | step: 100/263 | trn loss: 0.0055 | val loss: 0.0580\n","epoch: 21/100 | step: 200/263 | trn loss: 0.0046 | val loss: 0.0541\n","epoch: 22/100 | step: 100/263 | trn loss: 0.0038 | val loss: 0.0516\n","epoch: 22/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0575\n","epoch: 23/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0505\n","epoch: 23/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0520\n","epoch: 24/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0516\n","epoch: 24/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0513\n","epoch: 25/100 | step: 100/263 | trn loss: 0.0042 | val loss: 0.0568\n","epoch: 25/100 | step: 200/263 | trn loss: 0.0040 | val loss: 0.0589\n","epoch: 26/100 | step: 100/263 | trn loss: 0.0104 | val loss: 0.0947\n","epoch: 26/100 | step: 200/263 | trn loss: 0.0167 | val loss: 0.0685\n","epoch: 27/100 | step: 100/263 | trn loss: 0.0143 | val loss: 0.0730\n","epoch: 27/100 | step: 200/263 | trn loss: 0.0122 | val loss: 0.0598\n","epoch: 28/100 | step: 100/263 | trn loss: 0.0060 | val loss: 0.0528\n","epoch: 28/100 | step: 200/263 | trn loss: 0.0053 | val loss: 0.0557\n","epoch: 29/100 | step: 100/263 | trn loss: 0.0045 | val loss: 0.0506\n","epoch: 29/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0507\n","epoch: 30/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0480\n","epoch: 30/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0521\n","epoch: 31/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0493\n","epoch: 31/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0513\n","epoch: 32/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0483\n","epoch: 32/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0533\n","epoch: 33/100 | step: 100/263 | trn loss: 0.0038 | val loss: 0.0523\n","epoch: 33/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0519\n","epoch: 34/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0542\n","epoch: 34/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0487\n","epoch: 35/100 | step: 100/263 | trn loss: 0.0082 | val loss: 0.0660\n","epoch: 35/100 | step: 200/263 | trn loss: 0.0177 | val loss: 0.0736\n","epoch: 36/100 | step: 100/263 | trn loss: 0.0179 | val loss: 0.0687\n","epoch: 36/100 | step: 200/263 | trn loss: 0.0128 | val loss: 0.0599\n","epoch: 37/100 | step: 100/263 | trn loss: 0.0048 | val loss: 0.0531\n","epoch: 37/100 | step: 200/263 | trn loss: 0.0049 | val loss: 0.0560\n","epoch: 38/100 | step: 100/263 | trn loss: 0.0052 | val loss: 0.0525\n","epoch: 38/100 | step: 200/263 | trn loss: 0.0049 | val loss: 0.0516\n","epoch: 39/100 | step: 100/263 | trn loss: 0.0055 | val loss: 0.0518\n","epoch: 39/100 | step: 200/263 | trn loss: 0.0052 | val loss: 0.0546\n","epoch: 40/100 | step: 100/263 | trn loss: 0.0046 | val loss: 0.0546\n","epoch: 40/100 | step: 200/263 | trn loss: 0.0043 | val loss: 0.0489\n","epoch: 41/100 | step: 100/263 | trn loss: 0.0056 | val loss: 0.0608\n","epoch: 41/100 | step: 200/263 | trn loss: 0.0070 | val loss: 0.0700\n","epoch: 42/100 | step: 100/263 | trn loss: 0.0054 | val loss: 0.0556\n","epoch: 42/100 | step: 200/263 | trn loss: 0.0050 | val loss: 0.0581\n","epoch: 43/100 | step: 100/263 | trn loss: 0.0069 | val loss: 0.0688\n","epoch: 43/100 | step: 200/263 | trn loss: 0.0080 | val loss: 0.0547\n","epoch: 44/100 | step: 100/263 | trn loss: 0.0056 | val loss: 0.0593\n","epoch: 44/100 | step: 200/263 | trn loss: 0.0052 | val loss: 0.0536\n","epoch: 45/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0510\n","epoch: 45/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0500\n","epoch: 46/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0517\n","epoch: 46/100 | step: 200/263 | trn loss: 0.0045 | val loss: 0.0566\n","epoch: 47/100 | step: 100/263 | trn loss: 0.0094 | val loss: 0.0686\n","epoch: 47/100 | step: 200/263 | trn loss: 0.0116 | val loss: 0.0607\n","epoch: 48/100 | step: 100/263 | trn loss: 0.0098 | val loss: 0.0598\n","epoch: 48/100 | step: 200/263 | trn loss: 0.0058 | val loss: 0.0537\n","epoch: 49/100 | step: 100/263 | trn loss: 0.0077 | val loss: 0.0645\n","epoch: 49/100 | step: 200/263 | trn loss: 0.0083 | val loss: 0.0629\n","epoch: 50/100 | step: 100/263 | trn loss: 0.0048 | val loss: 0.0497\n","epoch: 50/100 | step: 200/263 | trn loss: 0.0050 | val loss: 0.0560\n","epoch: 51/100 | step: 100/263 | trn loss: 0.0055 | val loss: 0.0540\n","epoch: 51/100 | step: 200/263 | trn loss: 0.0042 | val loss: 0.0510\n","epoch: 52/100 | step: 100/263 | trn loss: 0.0040 | val loss: 0.0549\n","epoch: 52/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0506\n","epoch: 53/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0509\n","epoch: 53/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0518\n","epoch: 54/100 | step: 100/263 | trn loss: 0.0038 | val loss: 0.0501\n","epoch: 54/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0491\n","epoch: 55/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0536\n","epoch: 55/100 | step: 200/263 | trn loss: 0.0041 | val loss: 0.0544\n","epoch: 56/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0495\n","epoch: 56/100 | step: 200/263 | trn loss: 0.0141 | val loss: 0.0826\n","epoch: 57/100 | step: 100/263 | trn loss: 0.0136 | val loss: 0.0696\n","epoch: 57/100 | step: 200/263 | trn loss: 0.0120 | val loss: 0.0639\n","epoch: 58/100 | step: 100/263 | trn loss: 0.0063 | val loss: 0.0592\n","epoch: 58/100 | step: 200/263 | trn loss: 0.0052 | val loss: 0.0596\n","epoch: 59/100 | step: 100/263 | trn loss: 0.0041 | val loss: 0.0530\n","epoch: 59/100 | step: 200/263 | trn loss: 0.0049 | val loss: 0.0586\n","epoch: 60/100 | step: 100/263 | trn loss: 0.0051 | val loss: 0.0528\n","epoch: 60/100 | step: 200/263 | trn loss: 0.0048 | val loss: 0.0589\n","epoch: 61/100 | step: 100/263 | trn loss: 0.0055 | val loss: 0.0555\n","epoch: 61/100 | step: 200/263 | trn loss: 0.0048 | val loss: 0.0563\n","epoch: 62/100 | step: 100/263 | trn loss: 0.0057 | val loss: 0.0579\n","epoch: 62/100 | step: 200/263 | trn loss: 0.0052 | val loss: 0.0568\n","epoch: 63/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0525\n","epoch: 63/100 | step: 200/263 | trn loss: 0.0040 | val loss: 0.0501\n","epoch: 64/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0497\n","epoch: 64/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0512\n","epoch: 65/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0526\n","epoch: 65/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0512\n","epoch: 66/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0512\n","epoch: 66/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0520\n","epoch: 67/100 | step: 100/263 | trn loss: 0.0041 | val loss: 0.0513\n","epoch: 67/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0517\n","epoch: 68/100 | step: 100/263 | trn loss: 0.0075 | val loss: 0.0726\n","epoch: 68/100 | step: 200/263 | trn loss: 0.0120 | val loss: 0.0718\n","epoch: 69/100 | step: 100/263 | trn loss: 0.0157 | val loss: 0.0690\n","epoch: 69/100 | step: 200/263 | trn loss: 0.0149 | val loss: 0.0608\n","epoch: 70/100 | step: 100/263 | trn loss: 0.0071 | val loss: 0.0552\n","epoch: 70/100 | step: 200/263 | trn loss: 0.0057 | val loss: 0.0536\n","epoch: 71/100 | step: 100/263 | trn loss: 0.0035 | val loss: 0.0547\n","epoch: 71/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0538\n","epoch: 72/100 | step: 100/263 | trn loss: 0.0047 | val loss: 0.0575\n","epoch: 72/100 | step: 200/263 | trn loss: 0.0051 | val loss: 0.0657\n","epoch: 73/100 | step: 100/263 | trn loss: 0.0068 | val loss: 0.0588\n","epoch: 73/100 | step: 200/263 | trn loss: 0.0049 | val loss: 0.0551\n","epoch: 74/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0506\n","epoch: 74/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0517\n","epoch: 75/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0482\n","epoch: 75/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0522\n","epoch: 76/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0500\n","epoch: 76/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0521\n","epoch: 77/100 | step: 100/263 | trn loss: 0.0047 | val loss: 0.0568\n","epoch: 77/100 | step: 200/263 | trn loss: 0.0053 | val loss: 0.0523\n","epoch: 78/100 | step: 100/263 | trn loss: 0.0092 | val loss: 0.0636\n","epoch: 78/100 | step: 200/263 | trn loss: 0.0063 | val loss: 0.0579\n","epoch: 79/100 | step: 100/263 | trn loss: 0.0045 | val loss: 0.0533\n","epoch: 79/100 | step: 200/263 | trn loss: 0.0045 | val loss: 0.0489\n","epoch: 80/100 | step: 100/263 | trn loss: 0.0050 | val loss: 0.0569\n","epoch: 80/100 | step: 200/263 | trn loss: 0.0050 | val loss: 0.0507\n","epoch: 81/100 | step: 100/263 | trn loss: 0.0041 | val loss: 0.0544\n","epoch: 81/100 | step: 200/263 | trn loss: 0.0049 | val loss: 0.0561\n","epoch: 82/100 | step: 100/263 | trn loss: 0.0115 | val loss: 0.0605\n","epoch: 82/100 | step: 200/263 | trn loss: 0.0095 | val loss: 0.0593\n","epoch: 83/100 | step: 100/263 | trn loss: 0.0060 | val loss: 0.0558\n","epoch: 83/100 | step: 200/263 | trn loss: 0.0058 | val loss: 0.0562\n","epoch: 84/100 | step: 100/263 | trn loss: 0.0093 | val loss: 0.0636\n","epoch: 84/100 | step: 200/263 | trn loss: 0.0058 | val loss: 0.0539\n","epoch: 85/100 | step: 100/263 | trn loss: 0.0045 | val loss: 0.0545\n","epoch: 85/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0495\n","epoch: 86/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0503\n","epoch: 86/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0504\n","epoch: 87/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0504\n","epoch: 87/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0505\n","epoch: 88/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0535\n","epoch: 88/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0522\n","epoch: 89/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0531\n","epoch: 89/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0476\n","epoch: 90/100 | step: 100/263 | trn loss: 0.0076 | val loss: 0.0606\n","epoch: 90/100 | step: 200/263 | trn loss: 0.0086 | val loss: 0.0656\n","epoch: 91/100 | step: 100/263 | trn loss: 0.0099 | val loss: 0.0683\n","epoch: 91/100 | step: 200/263 | trn loss: 0.0108 | val loss: 0.0574\n","epoch: 92/100 | step: 100/263 | trn loss: 0.0047 | val loss: 0.0593\n","epoch: 92/100 | step: 200/263 | trn loss: 0.0050 | val loss: 0.0591\n","epoch: 93/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0532\n","epoch: 93/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0500\n","epoch: 94/100 | step: 100/263 | trn loss: 0.0046 | val loss: 0.0563\n","epoch: 94/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0522\n","epoch: 95/100 | step: 100/263 | trn loss: 0.0035 | val loss: 0.0550\n","epoch: 95/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0538\n","epoch: 96/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0527\n","epoch: 96/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0494\n","epoch: 97/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0521\n","epoch: 97/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0577\n","epoch: 98/100 | step: 100/263 | trn loss: 0.0036 | val loss: 0.0498\n","epoch: 98/100 | step: 200/263 | trn loss: 0.0038 | val loss: 0.0513\n","epoch: 99/100 | step: 100/263 | trn loss: 0.0039 | val loss: 0.0550\n","epoch: 99/100 | step: 200/263 | trn loss: 0.0037 | val loss: 0.0530\n","epoch: 100/100 | step: 100/263 | trn loss: 0.0037 | val loss: 0.0558\n","epoch: 100/100 | step: 200/263 | trn loss: 0.0039 | val loss: 0.0533\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NFqOkdRO1R7j","colab_type":"text"},"source":["가중치 초기화를 없애고 batch norm만 적용했더니 오히려 성능이 더 올랐다"]},{"cell_type":"code","metadata":{"id":"KEFGUWqp1aNE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"28d31030-b6d2-4536-fcf7-6cd31b4e03c8","executionInfo":{"status":"ok","timestamp":1584454639443,"user_tz":-540,"elapsed":239434,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["# 4 layers, dropout(0.5)\n","trn_loss_list = []\n","val_loss_list = []\n","num_epochs = 80\n","num_batches = len(traindataloader)\n","for epoch in range(num_epochs):  # loop over the dataset multiple times\n","\n","    trn_loss = 0.0\n","    for i, data in enumerate(traindataloader):\n","        inputs, labels = data\n","        inputs = torch.tensor(inputs, device=device).float()\n","        labels = torch.tensor(labels, device=device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # print(type(inputs), type(labels), type(outputs))\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        # print(type(inputs), type(labels), type(outputs))\n","        loss = criterion(outputs, labels)\n","        # back propagation\n","        loss.backward()\n","        # weight update\n","        optimizer.step()\n","        # print statistics\n","        trn_loss += loss.item()\n","\n","        # del(memory issue)\n","        del loss\n","        del outputs\n","\n","        # 학습과정 출력\n","        if (i+1) % 100 == 0: # every 100 mini-batches\n","            with torch.no_grad(): # very very very very important!!!\n","                val_loss = 0.0\n","                for j, val in enumerate(validdataloader):\n","                    val_x, val_label = val\n","                    val_x = torch.tensor(val_x, device=device).float()\n","                    val_label = torch.tensor(val_label, device=device)\n","                    val_output = net(val_x)\n","                    v_loss = criterion(val_output, val_label)\n","                    val_loss += v_loss\n","                       \n","            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n","                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(validdataloader)\n","            ))            \n","            \n","            trn_loss_list.append(trn_loss/100)\n","            val_loss_list.append(val_loss/len(validdataloader))\n","            trn_loss = 0.0        \n","\n","       \n","\n","    #print(\"Epoch : {} loss: {}\".format(epoch, running_loss))\n","print('Finished Training')"],"execution_count":104,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 1/80 | step: 100/263 | trn loss: 2.2212 | val loss: 1.9678\n","epoch: 1/80 | step: 200/263 | trn loss: 1.3725 | val loss: 0.8850\n","epoch: 2/80 | step: 100/263 | trn loss: 0.5078 | val loss: 0.4254\n","epoch: 2/80 | step: 200/263 | trn loss: 0.4014 | val loss: 0.3510\n","epoch: 3/80 | step: 100/263 | trn loss: 0.3136 | val loss: 0.2901\n","epoch: 3/80 | step: 200/263 | trn loss: 0.2822 | val loss: 0.2763\n","epoch: 4/80 | step: 100/263 | trn loss: 0.2444 | val loss: 0.2400\n","epoch: 4/80 | step: 200/263 | trn loss: 0.2415 | val loss: 0.2250\n","epoch: 5/80 | step: 100/263 | trn loss: 0.2154 | val loss: 0.2153\n","epoch: 5/80 | step: 200/263 | trn loss: 0.2093 | val loss: 0.2093\n","epoch: 6/80 | step: 100/263 | trn loss: 0.1975 | val loss: 0.1921\n","epoch: 6/80 | step: 200/263 | trn loss: 0.1866 | val loss: 0.1845\n","epoch: 7/80 | step: 100/263 | trn loss: 0.1733 | val loss: 0.1796\n","epoch: 7/80 | step: 200/263 | trn loss: 0.1746 | val loss: 0.1726\n","epoch: 8/80 | step: 100/263 | trn loss: 0.1537 | val loss: 0.1714\n","epoch: 8/80 | step: 200/263 | trn loss: 0.1648 | val loss: 0.1688\n","epoch: 9/80 | step: 100/263 | trn loss: 0.1496 | val loss: 0.1658\n","epoch: 9/80 | step: 200/263 | trn loss: 0.1467 | val loss: 0.1601\n","epoch: 10/80 | step: 100/263 | trn loss: 0.1390 | val loss: 0.1594\n","epoch: 10/80 | step: 200/263 | trn loss: 0.1420 | val loss: 0.1523\n","epoch: 11/80 | step: 100/263 | trn loss: 0.1285 | val loss: 0.1458\n","epoch: 11/80 | step: 200/263 | trn loss: 0.1408 | val loss: 0.1469\n","epoch: 12/80 | step: 100/263 | trn loss: 0.1260 | val loss: 0.1384\n","epoch: 12/80 | step: 200/263 | trn loss: 0.1175 | val loss: 0.1403\n","epoch: 13/80 | step: 100/263 | trn loss: 0.1224 | val loss: 0.1435\n","epoch: 13/80 | step: 200/263 | trn loss: 0.1165 | val loss: 0.1358\n","epoch: 14/80 | step: 100/263 | trn loss: 0.1109 | val loss: 0.1403\n","epoch: 14/80 | step: 200/263 | trn loss: 0.1123 | val loss: 0.1318\n","epoch: 15/80 | step: 100/263 | trn loss: 0.1028 | val loss: 0.1351\n","epoch: 15/80 | step: 200/263 | trn loss: 0.1096 | val loss: 0.1358\n","epoch: 16/80 | step: 100/263 | trn loss: 0.1089 | val loss: 0.1318\n","epoch: 16/80 | step: 200/263 | trn loss: 0.0988 | val loss: 0.1250\n","epoch: 17/80 | step: 100/263 | trn loss: 0.0962 | val loss: 0.1246\n","epoch: 17/80 | step: 200/263 | trn loss: 0.1037 | val loss: 0.1260\n","epoch: 18/80 | step: 100/263 | trn loss: 0.0927 | val loss: 0.1193\n","epoch: 18/80 | step: 200/263 | trn loss: 0.1000 | val loss: 0.1179\n","epoch: 19/80 | step: 100/263 | trn loss: 0.0939 | val loss: 0.1165\n","epoch: 19/80 | step: 200/263 | trn loss: 0.0889 | val loss: 0.1187\n","epoch: 20/80 | step: 100/263 | trn loss: 0.0859 | val loss: 0.1133\n","epoch: 20/80 | step: 200/263 | trn loss: 0.0873 | val loss: 0.1151\n","epoch: 21/80 | step: 100/263 | trn loss: 0.0826 | val loss: 0.1174\n","epoch: 21/80 | step: 200/263 | trn loss: 0.0869 | val loss: 0.1136\n","epoch: 22/80 | step: 100/263 | trn loss: 0.0820 | val loss: 0.1146\n","epoch: 22/80 | step: 200/263 | trn loss: 0.0813 | val loss: 0.1130\n","epoch: 23/80 | step: 100/263 | trn loss: 0.0754 | val loss: 0.1164\n","epoch: 23/80 | step: 200/263 | trn loss: 0.0813 | val loss: 0.1130\n","epoch: 24/80 | step: 100/263 | trn loss: 0.0738 | val loss: 0.1095\n","epoch: 24/80 | step: 200/263 | trn loss: 0.0775 | val loss: 0.1034\n","epoch: 25/80 | step: 100/263 | trn loss: 0.0749 | val loss: 0.1070\n","epoch: 25/80 | step: 200/263 | trn loss: 0.0732 | val loss: 0.1055\n","epoch: 26/80 | step: 100/263 | trn loss: 0.0769 | val loss: 0.1017\n","epoch: 26/80 | step: 200/263 | trn loss: 0.0702 | val loss: 0.1004\n","epoch: 27/80 | step: 100/263 | trn loss: 0.0709 | val loss: 0.1045\n","epoch: 27/80 | step: 200/263 | trn loss: 0.0745 | val loss: 0.1032\n","epoch: 28/80 | step: 100/263 | trn loss: 0.0684 | val loss: 0.1003\n","epoch: 28/80 | step: 200/263 | trn loss: 0.0679 | val loss: 0.1029\n","epoch: 29/80 | step: 100/263 | trn loss: 0.0663 | val loss: 0.1005\n","epoch: 29/80 | step: 200/263 | trn loss: 0.0634 | val loss: 0.1038\n","epoch: 30/80 | step: 100/263 | trn loss: 0.0628 | val loss: 0.0924\n","epoch: 30/80 | step: 200/263 | trn loss: 0.0632 | val loss: 0.0964\n","epoch: 31/80 | step: 100/263 | trn loss: 0.0610 | val loss: 0.0968\n","epoch: 31/80 | step: 200/263 | trn loss: 0.0626 | val loss: 0.0974\n","epoch: 32/80 | step: 100/263 | trn loss: 0.0612 | val loss: 0.0964\n","epoch: 32/80 | step: 200/263 | trn loss: 0.0632 | val loss: 0.0970\n","epoch: 33/80 | step: 100/263 | trn loss: 0.0579 | val loss: 0.0991\n","epoch: 33/80 | step: 200/263 | trn loss: 0.0632 | val loss: 0.0958\n","epoch: 34/80 | step: 100/263 | trn loss: 0.0543 | val loss: 0.0954\n","epoch: 34/80 | step: 200/263 | trn loss: 0.0619 | val loss: 0.0948\n","epoch: 35/80 | step: 100/263 | trn loss: 0.0560 | val loss: 0.0906\n","epoch: 35/80 | step: 200/263 | trn loss: 0.0545 | val loss: 0.0938\n","epoch: 36/80 | step: 100/263 | trn loss: 0.0535 | val loss: 0.0934\n","epoch: 36/80 | step: 200/263 | trn loss: 0.0585 | val loss: 0.0936\n","epoch: 37/80 | step: 100/263 | trn loss: 0.0519 | val loss: 0.0910\n","epoch: 37/80 | step: 200/263 | trn loss: 0.0540 | val loss: 0.0914\n","epoch: 38/80 | step: 100/263 | trn loss: 0.0486 | val loss: 0.0945\n","epoch: 38/80 | step: 200/263 | trn loss: 0.0536 | val loss: 0.0947\n","epoch: 39/80 | step: 100/263 | trn loss: 0.0479 | val loss: 0.0889\n","epoch: 39/80 | step: 200/263 | trn loss: 0.0538 | val loss: 0.0929\n","epoch: 40/80 | step: 100/263 | trn loss: 0.0521 | val loss: 0.0902\n","epoch: 40/80 | step: 200/263 | trn loss: 0.0486 | val loss: 0.0877\n","epoch: 41/80 | step: 100/263 | trn loss: 0.0534 | val loss: 0.0820\n","epoch: 41/80 | step: 200/263 | trn loss: 0.0477 | val loss: 0.0830\n","epoch: 42/80 | step: 100/263 | trn loss: 0.0480 | val loss: 0.0883\n","epoch: 42/80 | step: 200/263 | trn loss: 0.0499 | val loss: 0.0901\n","epoch: 43/80 | step: 100/263 | trn loss: 0.0429 | val loss: 0.0957\n","epoch: 43/80 | step: 200/263 | trn loss: 0.0499 | val loss: 0.0843\n","epoch: 44/80 | step: 100/263 | trn loss: 0.0462 | val loss: 0.0879\n","epoch: 44/80 | step: 200/263 | trn loss: 0.0521 | val loss: 0.0918\n","epoch: 45/80 | step: 100/263 | trn loss: 0.0448 | val loss: 0.0814\n","epoch: 45/80 | step: 200/263 | trn loss: 0.0448 | val loss: 0.0860\n","epoch: 46/80 | step: 100/263 | trn loss: 0.0441 | val loss: 0.0848\n","epoch: 46/80 | step: 200/263 | trn loss: 0.0487 | val loss: 0.0859\n","epoch: 47/80 | step: 100/263 | trn loss: 0.0444 | val loss: 0.0898\n","epoch: 47/80 | step: 200/263 | trn loss: 0.0457 | val loss: 0.0852\n","epoch: 48/80 | step: 100/263 | trn loss: 0.0454 | val loss: 0.0883\n","epoch: 48/80 | step: 200/263 | trn loss: 0.0404 | val loss: 0.0840\n","epoch: 49/80 | step: 100/263 | trn loss: 0.0427 | val loss: 0.0863\n","epoch: 49/80 | step: 200/263 | trn loss: 0.0391 | val loss: 0.0868\n","epoch: 50/80 | step: 100/263 | trn loss: 0.0405 | val loss: 0.0861\n","epoch: 50/80 | step: 200/263 | trn loss: 0.0414 | val loss: 0.0881\n","epoch: 51/80 | step: 100/263 | trn loss: 0.0411 | val loss: 0.0899\n","epoch: 51/80 | step: 200/263 | trn loss: 0.0432 | val loss: 0.0831\n","epoch: 52/80 | step: 100/263 | trn loss: 0.0411 | val loss: 0.0783\n","epoch: 52/80 | step: 200/263 | trn loss: 0.0424 | val loss: 0.0865\n","epoch: 53/80 | step: 100/263 | trn loss: 0.0395 | val loss: 0.0821\n","epoch: 53/80 | step: 200/263 | trn loss: 0.0397 | val loss: 0.0849\n","epoch: 54/80 | step: 100/263 | trn loss: 0.0378 | val loss: 0.0840\n","epoch: 54/80 | step: 200/263 | trn loss: 0.0395 | val loss: 0.0816\n","epoch: 55/80 | step: 100/263 | trn loss: 0.0434 | val loss: 0.0833\n","epoch: 55/80 | step: 200/263 | trn loss: 0.0372 | val loss: 0.0837\n","epoch: 56/80 | step: 100/263 | trn loss: 0.0391 | val loss: 0.0873\n","epoch: 56/80 | step: 200/263 | trn loss: 0.0340 | val loss: 0.0826\n","epoch: 57/80 | step: 100/263 | trn loss: 0.0409 | val loss: 0.0759\n","epoch: 57/80 | step: 200/263 | trn loss: 0.0390 | val loss: 0.0854\n","epoch: 58/80 | step: 100/263 | trn loss: 0.0414 | val loss: 0.0853\n","epoch: 58/80 | step: 200/263 | trn loss: 0.0393 | val loss: 0.0771\n","epoch: 59/80 | step: 100/263 | trn loss: 0.0392 | val loss: 0.0844\n","epoch: 59/80 | step: 200/263 | trn loss: 0.0383 | val loss: 0.0851\n","epoch: 60/80 | step: 100/263 | trn loss: 0.0356 | val loss: 0.0791\n","epoch: 60/80 | step: 200/263 | trn loss: 0.0409 | val loss: 0.0791\n","epoch: 61/80 | step: 100/263 | trn loss: 0.0342 | val loss: 0.0838\n","epoch: 61/80 | step: 200/263 | trn loss: 0.0398 | val loss: 0.0816\n","epoch: 62/80 | step: 100/263 | trn loss: 0.0375 | val loss: 0.0779\n","epoch: 62/80 | step: 200/263 | trn loss: 0.0419 | val loss: 0.0796\n","epoch: 63/80 | step: 100/263 | trn loss: 0.0337 | val loss: 0.0823\n","epoch: 63/80 | step: 200/263 | trn loss: 0.0396 | val loss: 0.0790\n","epoch: 64/80 | step: 100/263 | trn loss: 0.0355 | val loss: 0.0844\n","epoch: 64/80 | step: 200/263 | trn loss: 0.0351 | val loss: 0.0809\n","epoch: 65/80 | step: 100/263 | trn loss: 0.0390 | val loss: 0.0783\n","epoch: 65/80 | step: 200/263 | trn loss: 0.0342 | val loss: 0.0827\n","epoch: 66/80 | step: 100/263 | trn loss: 0.0313 | val loss: 0.0806\n","epoch: 66/80 | step: 200/263 | trn loss: 0.0340 | val loss: 0.0793\n","epoch: 67/80 | step: 100/263 | trn loss: 0.0323 | val loss: 0.0819\n","epoch: 67/80 | step: 200/263 | trn loss: 0.0363 | val loss: 0.0802\n","epoch: 68/80 | step: 100/263 | trn loss: 0.0323 | val loss: 0.0830\n","epoch: 68/80 | step: 200/263 | trn loss: 0.0368 | val loss: 0.0730\n","epoch: 69/80 | step: 100/263 | trn loss: 0.0318 | val loss: 0.0769\n","epoch: 69/80 | step: 200/263 | trn loss: 0.0364 | val loss: 0.0811\n","epoch: 70/80 | step: 100/263 | trn loss: 0.0333 | val loss: 0.0800\n","epoch: 70/80 | step: 200/263 | trn loss: 0.0324 | val loss: 0.0791\n","epoch: 71/80 | step: 100/263 | trn loss: 0.0329 | val loss: 0.0760\n","epoch: 71/80 | step: 200/263 | trn loss: 0.0326 | val loss: 0.0806\n","epoch: 72/80 | step: 100/263 | trn loss: 0.0342 | val loss: 0.0840\n","epoch: 72/80 | step: 200/263 | trn loss: 0.0340 | val loss: 0.0758\n","epoch: 73/80 | step: 100/263 | trn loss: 0.0320 | val loss: 0.0792\n","epoch: 73/80 | step: 200/263 | trn loss: 0.0344 | val loss: 0.0803\n","epoch: 74/80 | step: 100/263 | trn loss: 0.0318 | val loss: 0.0815\n","epoch: 74/80 | step: 200/263 | trn loss: 0.0305 | val loss: 0.0805\n","epoch: 75/80 | step: 100/263 | trn loss: 0.0332 | val loss: 0.0807\n","epoch: 75/80 | step: 200/263 | trn loss: 0.0327 | val loss: 0.0797\n","epoch: 76/80 | step: 100/263 | trn loss: 0.0296 | val loss: 0.0814\n","epoch: 76/80 | step: 200/263 | trn loss: 0.0330 | val loss: 0.0807\n","epoch: 77/80 | step: 100/263 | trn loss: 0.0312 | val loss: 0.0858\n","epoch: 77/80 | step: 200/263 | trn loss: 0.0314 | val loss: 0.0850\n","epoch: 78/80 | step: 100/263 | trn loss: 0.0311 | val loss: 0.0814\n","epoch: 78/80 | step: 200/263 | trn loss: 0.0330 | val loss: 0.0844\n","epoch: 79/80 | step: 100/263 | trn loss: 0.0305 | val loss: 0.0765\n","epoch: 79/80 | step: 200/263 | trn loss: 0.0312 | val loss: 0.0795\n","epoch: 80/80 | step: 100/263 | trn loss: 0.0310 | val loss: 0.0790\n","epoch: 80/80 | step: 200/263 | trn loss: 0.0322 | val loss: 0.0754\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2-zN5VTt1-V6","colab_type":"text"},"source":["가중치 초기화 없애고 dropout만 적용했더니 성능이 batch norm만 적용했을때보다 낮다"]},{"cell_type":"code","metadata":{"id":"vAouFl4o2R82","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d0b8491e-5707-4a01-87d1-7404e173b6fa","executionInfo":{"status":"ok","timestamp":1584454974067,"user_tz":-540,"elapsed":293507,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["# 4 layers, batch normalization, dropout(0.4)\n","trn_loss_list = []\n","val_loss_list = []\n","num_epochs = 80\n","num_batches = len(traindataloader)\n","for epoch in range(num_epochs):  # loop over the dataset multiple times\n","\n","    trn_loss = 0.0\n","    for i, data in enumerate(traindataloader):\n","        inputs, labels = data\n","        inputs = torch.tensor(inputs, device=device).float()\n","        labels = torch.tensor(labels, device=device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # print(type(inputs), type(labels), type(outputs))\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        # print(type(inputs), type(labels), type(outputs))\n","        loss = criterion(outputs, labels)\n","        # back propagation\n","        loss.backward()\n","        # weight update\n","        optimizer.step()\n","        # print statistics\n","        trn_loss += loss.item()\n","\n","        # del(memory issue)\n","        del loss\n","        del outputs\n","\n","        # 학습과정 출력\n","        if (i+1) % 100 == 0: # every 100 mini-batches\n","            with torch.no_grad(): # very very very very important!!!\n","                val_loss = 0.0\n","                for j, val in enumerate(validdataloader):\n","                    val_x, val_label = val\n","                    val_x = torch.tensor(val_x, device=device).float()\n","                    val_label = torch.tensor(val_label, device=device)\n","                    val_output = net(val_x)\n","                    v_loss = criterion(val_output, val_label)\n","                    val_loss += v_loss\n","                       \n","            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n","                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(validdataloader)\n","            ))            \n","            \n","            trn_loss_list.append(trn_loss/100)\n","            val_loss_list.append(val_loss/len(validdataloader))\n","            trn_loss = 0.0        \n","\n","       \n","\n","    #print(\"Epoch : {} loss: {}\".format(epoch, running_loss))\n","print('Finished Training')"],"execution_count":110,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 1/80 | step: 100/263 | trn loss: 1.7695 | val loss: 1.2485\n","epoch: 1/80 | step: 200/263 | trn loss: 0.9898 | val loss: 0.7872\n","epoch: 2/80 | step: 100/263 | trn loss: 0.5503 | val loss: 0.4886\n","epoch: 2/80 | step: 200/263 | trn loss: 0.4247 | val loss: 0.3937\n","epoch: 3/80 | step: 100/263 | trn loss: 0.3079 | val loss: 0.3058\n","epoch: 3/80 | step: 200/263 | trn loss: 0.2758 | val loss: 0.2787\n","epoch: 4/80 | step: 100/263 | trn loss: 0.2224 | val loss: 0.2370\n","epoch: 4/80 | step: 200/263 | trn loss: 0.2135 | val loss: 0.2218\n","epoch: 5/80 | step: 100/263 | trn loss: 0.1796 | val loss: 0.2001\n","epoch: 5/80 | step: 200/263 | trn loss: 0.1674 | val loss: 0.1980\n","epoch: 6/80 | step: 100/263 | trn loss: 0.1436 | val loss: 0.1823\n","epoch: 6/80 | step: 200/263 | trn loss: 0.1457 | val loss: 0.1771\n","epoch: 7/80 | step: 100/263 | trn loss: 0.1235 | val loss: 0.1707\n","epoch: 7/80 | step: 200/263 | trn loss: 0.1249 | val loss: 0.1532\n","epoch: 8/80 | step: 100/263 | trn loss: 0.1109 | val loss: 0.1492\n","epoch: 8/80 | step: 200/263 | trn loss: 0.1078 | val loss: 0.1536\n","epoch: 9/80 | step: 100/263 | trn loss: 0.0981 | val loss: 0.1379\n","epoch: 9/80 | step: 200/263 | trn loss: 0.1016 | val loss: 0.1423\n","epoch: 10/80 | step: 100/263 | trn loss: 0.0864 | val loss: 0.1264\n","epoch: 10/80 | step: 200/263 | trn loss: 0.0876 | val loss: 0.1269\n","epoch: 11/80 | step: 100/263 | trn loss: 0.0760 | val loss: 0.1254\n","epoch: 11/80 | step: 200/263 | trn loss: 0.0810 | val loss: 0.1219\n","epoch: 12/80 | step: 100/263 | trn loss: 0.0725 | val loss: 0.1200\n","epoch: 12/80 | step: 200/263 | trn loss: 0.0712 | val loss: 0.1136\n","epoch: 13/80 | step: 100/263 | trn loss: 0.0647 | val loss: 0.1151\n","epoch: 13/80 | step: 200/263 | trn loss: 0.0659 | val loss: 0.1139\n","epoch: 14/80 | step: 100/263 | trn loss: 0.0590 | val loss: 0.1090\n","epoch: 14/80 | step: 200/263 | trn loss: 0.0548 | val loss: 0.1089\n","epoch: 15/80 | step: 100/263 | trn loss: 0.0551 | val loss: 0.1070\n","epoch: 15/80 | step: 200/263 | trn loss: 0.0583 | val loss: 0.1097\n","epoch: 16/80 | step: 100/263 | trn loss: 0.0527 | val loss: 0.1033\n","epoch: 16/80 | step: 200/263 | trn loss: 0.0500 | val loss: 0.0969\n","epoch: 17/80 | step: 100/263 | trn loss: 0.0465 | val loss: 0.0950\n","epoch: 17/80 | step: 200/263 | trn loss: 0.0458 | val loss: 0.0951\n","epoch: 18/80 | step: 100/263 | trn loss: 0.0432 | val loss: 0.0989\n","epoch: 18/80 | step: 200/263 | trn loss: 0.0422 | val loss: 0.0936\n","epoch: 19/80 | step: 100/263 | trn loss: 0.0413 | val loss: 0.0964\n","epoch: 19/80 | step: 200/263 | trn loss: 0.0395 | val loss: 0.0955\n","epoch: 20/80 | step: 100/263 | trn loss: 0.0383 | val loss: 0.0926\n","epoch: 20/80 | step: 200/263 | trn loss: 0.0380 | val loss: 0.0949\n","epoch: 21/80 | step: 100/263 | trn loss: 0.0370 | val loss: 0.0971\n","epoch: 21/80 | step: 200/263 | trn loss: 0.0346 | val loss: 0.0922\n","epoch: 22/80 | step: 100/263 | trn loss: 0.0368 | val loss: 0.0850\n","epoch: 22/80 | step: 200/263 | trn loss: 0.0361 | val loss: 0.0869\n","epoch: 23/80 | step: 100/263 | trn loss: 0.0296 | val loss: 0.0902\n","epoch: 23/80 | step: 200/263 | trn loss: 0.0305 | val loss: 0.0907\n","epoch: 24/80 | step: 100/263 | trn loss: 0.0324 | val loss: 0.0907\n","epoch: 24/80 | step: 200/263 | trn loss: 0.0318 | val loss: 0.0966\n","epoch: 25/80 | step: 100/263 | trn loss: 0.0267 | val loss: 0.0864\n","epoch: 25/80 | step: 200/263 | trn loss: 0.0314 | val loss: 0.0888\n","epoch: 26/80 | step: 100/263 | trn loss: 0.0309 | val loss: 0.0923\n","epoch: 26/80 | step: 200/263 | trn loss: 0.0289 | val loss: 0.0866\n","epoch: 27/80 | step: 100/263 | trn loss: 0.0300 | val loss: 0.0870\n","epoch: 27/80 | step: 200/263 | trn loss: 0.0299 | val loss: 0.0948\n","epoch: 28/80 | step: 100/263 | trn loss: 0.0291 | val loss: 0.0829\n","epoch: 28/80 | step: 200/263 | trn loss: 0.0272 | val loss: 0.0911\n","epoch: 29/80 | step: 100/263 | trn loss: 0.0253 | val loss: 0.0810\n","epoch: 29/80 | step: 200/263 | trn loss: 0.0255 | val loss: 0.0876\n","epoch: 30/80 | step: 100/263 | trn loss: 0.0210 | val loss: 0.0803\n","epoch: 30/80 | step: 200/263 | trn loss: 0.0259 | val loss: 0.0882\n","epoch: 31/80 | step: 100/263 | trn loss: 0.0248 | val loss: 0.0856\n","epoch: 31/80 | step: 200/263 | trn loss: 0.0243 | val loss: 0.0870\n","epoch: 32/80 | step: 100/263 | trn loss: 0.0220 | val loss: 0.0793\n","epoch: 32/80 | step: 200/263 | trn loss: 0.0258 | val loss: 0.0826\n","epoch: 33/80 | step: 100/263 | trn loss: 0.0226 | val loss: 0.0871\n","epoch: 33/80 | step: 200/263 | trn loss: 0.0223 | val loss: 0.0832\n","epoch: 34/80 | step: 100/263 | trn loss: 0.0218 | val loss: 0.0891\n","epoch: 34/80 | step: 200/263 | trn loss: 0.0224 | val loss: 0.0922\n","epoch: 35/80 | step: 100/263 | trn loss: 0.0203 | val loss: 0.0820\n","epoch: 35/80 | step: 200/263 | trn loss: 0.0238 | val loss: 0.0859\n","epoch: 36/80 | step: 100/263 | trn loss: 0.0223 | val loss: 0.0823\n","epoch: 36/80 | step: 200/263 | trn loss: 0.0237 | val loss: 0.0868\n","epoch: 37/80 | step: 100/263 | trn loss: 0.0198 | val loss: 0.0785\n","epoch: 37/80 | step: 200/263 | trn loss: 0.0221 | val loss: 0.0808\n","epoch: 38/80 | step: 100/263 | trn loss: 0.0199 | val loss: 0.0821\n","epoch: 38/80 | step: 200/263 | trn loss: 0.0190 | val loss: 0.0799\n","epoch: 39/80 | step: 100/263 | trn loss: 0.0197 | val loss: 0.0825\n","epoch: 39/80 | step: 200/263 | trn loss: 0.0180 | val loss: 0.0817\n","epoch: 40/80 | step: 100/263 | trn loss: 0.0192 | val loss: 0.0878\n","epoch: 40/80 | step: 200/263 | trn loss: 0.0219 | val loss: 0.0874\n","epoch: 41/80 | step: 100/263 | trn loss: 0.0191 | val loss: 0.0797\n","epoch: 41/80 | step: 200/263 | trn loss: 0.0192 | val loss: 0.0858\n","epoch: 42/80 | step: 100/263 | trn loss: 0.0195 | val loss: 0.0812\n","epoch: 42/80 | step: 200/263 | trn loss: 0.0188 | val loss: 0.0786\n","epoch: 43/80 | step: 100/263 | trn loss: 0.0188 | val loss: 0.0863\n","epoch: 43/80 | step: 200/263 | trn loss: 0.0196 | val loss: 0.0858\n","epoch: 44/80 | step: 100/263 | trn loss: 0.0175 | val loss: 0.0792\n","epoch: 44/80 | step: 200/263 | trn loss: 0.0186 | val loss: 0.0830\n","epoch: 45/80 | step: 100/263 | trn loss: 0.0179 | val loss: 0.0840\n","epoch: 45/80 | step: 200/263 | trn loss: 0.0206 | val loss: 0.0776\n","epoch: 46/80 | step: 100/263 | trn loss: 0.0183 | val loss: 0.0768\n","epoch: 46/80 | step: 200/263 | trn loss: 0.0193 | val loss: 0.0788\n","epoch: 47/80 | step: 100/263 | trn loss: 0.0183 | val loss: 0.0711\n","epoch: 47/80 | step: 200/263 | trn loss: 0.0193 | val loss: 0.0816\n","epoch: 48/80 | step: 100/263 | trn loss: 0.0169 | val loss: 0.0809\n","epoch: 48/80 | step: 200/263 | trn loss: 0.0160 | val loss: 0.0708\n","epoch: 49/80 | step: 100/263 | trn loss: 0.0150 | val loss: 0.0769\n","epoch: 49/80 | step: 200/263 | trn loss: 0.0174 | val loss: 0.0802\n","epoch: 50/80 | step: 100/263 | trn loss: 0.0165 | val loss: 0.0820\n","epoch: 50/80 | step: 200/263 | trn loss: 0.0165 | val loss: 0.0837\n","epoch: 51/80 | step: 100/263 | trn loss: 0.0174 | val loss: 0.0761\n","epoch: 51/80 | step: 200/263 | trn loss: 0.0169 | val loss: 0.0879\n","epoch: 52/80 | step: 100/263 | trn loss: 0.0166 | val loss: 0.0774\n","epoch: 52/80 | step: 200/263 | trn loss: 0.0163 | val loss: 0.0819\n","epoch: 53/80 | step: 100/263 | trn loss: 0.0176 | val loss: 0.0759\n","epoch: 53/80 | step: 200/263 | trn loss: 0.0169 | val loss: 0.0809\n","epoch: 54/80 | step: 100/263 | trn loss: 0.0170 | val loss: 0.0778\n","epoch: 54/80 | step: 200/263 | trn loss: 0.0172 | val loss: 0.0794\n","epoch: 55/80 | step: 100/263 | trn loss: 0.0173 | val loss: 0.0781\n","epoch: 55/80 | step: 200/263 | trn loss: 0.0180 | val loss: 0.0817\n","epoch: 56/80 | step: 100/263 | trn loss: 0.0137 | val loss: 0.0762\n","epoch: 56/80 | step: 200/263 | trn loss: 0.0174 | val loss: 0.0771\n","epoch: 57/80 | step: 100/263 | trn loss: 0.0154 | val loss: 0.0866\n","epoch: 57/80 | step: 200/263 | trn loss: 0.0176 | val loss: 0.0759\n","epoch: 58/80 | step: 100/263 | trn loss: 0.0153 | val loss: 0.0733\n","epoch: 58/80 | step: 200/263 | trn loss: 0.0162 | val loss: 0.0817\n","epoch: 59/80 | step: 100/263 | trn loss: 0.0140 | val loss: 0.0756\n","epoch: 59/80 | step: 200/263 | trn loss: 0.0135 | val loss: 0.0787\n","epoch: 60/80 | step: 100/263 | trn loss: 0.0160 | val loss: 0.0803\n","epoch: 60/80 | step: 200/263 | trn loss: 0.0160 | val loss: 0.0714\n","epoch: 61/80 | step: 100/263 | trn loss: 0.0143 | val loss: 0.0892\n","epoch: 61/80 | step: 200/263 | trn loss: 0.0170 | val loss: 0.0828\n","epoch: 62/80 | step: 100/263 | trn loss: 0.0150 | val loss: 0.0824\n","epoch: 62/80 | step: 200/263 | trn loss: 0.0155 | val loss: 0.0778\n","epoch: 63/80 | step: 100/263 | trn loss: 0.0158 | val loss: 0.0793\n","epoch: 63/80 | step: 200/263 | trn loss: 0.0154 | val loss: 0.0791\n","epoch: 64/80 | step: 100/263 | trn loss: 0.0149 | val loss: 0.0737\n","epoch: 64/80 | step: 200/263 | trn loss: 0.0152 | val loss: 0.0769\n","epoch: 65/80 | step: 100/263 | trn loss: 0.0151 | val loss: 0.0851\n","epoch: 65/80 | step: 200/263 | trn loss: 0.0158 | val loss: 0.0809\n","epoch: 66/80 | step: 100/263 | trn loss: 0.0150 | val loss: 0.0811\n","epoch: 66/80 | step: 200/263 | trn loss: 0.0158 | val loss: 0.0807\n","epoch: 67/80 | step: 100/263 | trn loss: 0.0131 | val loss: 0.0873\n","epoch: 67/80 | step: 200/263 | trn loss: 0.0173 | val loss: 0.0836\n","epoch: 68/80 | step: 100/263 | trn loss: 0.0160 | val loss: 0.0883\n","epoch: 68/80 | step: 200/263 | trn loss: 0.0151 | val loss: 0.0822\n","epoch: 69/80 | step: 100/263 | trn loss: 0.0147 | val loss: 0.0851\n","epoch: 69/80 | step: 200/263 | trn loss: 0.0149 | val loss: 0.0731\n","epoch: 70/80 | step: 100/263 | trn loss: 0.0137 | val loss: 0.0840\n","epoch: 70/80 | step: 200/263 | trn loss: 0.0143 | val loss: 0.0776\n","epoch: 71/80 | step: 100/263 | trn loss: 0.0145 | val loss: 0.0724\n","epoch: 71/80 | step: 200/263 | trn loss: 0.0154 | val loss: 0.0772\n","epoch: 72/80 | step: 100/263 | trn loss: 0.0164 | val loss: 0.0788\n","epoch: 72/80 | step: 200/263 | trn loss: 0.0162 | val loss: 0.0805\n","epoch: 73/80 | step: 100/263 | trn loss: 0.0130 | val loss: 0.0801\n","epoch: 73/80 | step: 200/263 | trn loss: 0.0130 | val loss: 0.0791\n","epoch: 74/80 | step: 100/263 | trn loss: 0.0128 | val loss: 0.0774\n","epoch: 74/80 | step: 200/263 | trn loss: 0.0115 | val loss: 0.0789\n","epoch: 75/80 | step: 100/263 | trn loss: 0.0130 | val loss: 0.0811\n","epoch: 75/80 | step: 200/263 | trn loss: 0.0141 | val loss: 0.0844\n","epoch: 76/80 | step: 100/263 | trn loss: 0.0170 | val loss: 0.0864\n","epoch: 76/80 | step: 200/263 | trn loss: 0.0141 | val loss: 0.0798\n","epoch: 77/80 | step: 100/263 | trn loss: 0.0151 | val loss: 0.0828\n","epoch: 77/80 | step: 200/263 | trn loss: 0.0126 | val loss: 0.0735\n","epoch: 78/80 | step: 100/263 | trn loss: 0.0124 | val loss: 0.0788\n","epoch: 78/80 | step: 200/263 | trn loss: 0.0130 | val loss: 0.0804\n","epoch: 79/80 | step: 100/263 | trn loss: 0.0120 | val loss: 0.0751\n","epoch: 79/80 | step: 200/263 | trn loss: 0.0128 | val loss: 0.0855\n","epoch: 80/80 | step: 100/263 | trn loss: 0.0120 | val loss: 0.0746\n","epoch: 80/80 | step: 200/263 | trn loss: 0.0140 | val loss: 0.0749\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3w9BndZC34KU","colab_type":"text"},"source":["batch norm과 dropout을 동시에 쓰면 보통 성능이 더 올라간다고 구글링에서 배웠는데 직접 여러번 실험해본 결과 이 경우엔 dropout말고 batch norm을 단독으로 썼을때가 성능이 더 높았다"]},{"cell_type":"markdown","metadata":{"id":"Q1BSf7Ta4o4c","colab_type":"text"},"source":["따라서, 가장 성능이 괜찮았던 4 layers(input-linear-relu-linear-relu-linear-relu-linear-softmax), batch normalization, adamoptimizer로 제출하기로 했다"]},{"cell_type":"markdown","metadata":{"id":"4sgosuzJy2gF","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"UGfmpOg5X7nq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"2854d94b-6673-4a44-cceb-597d9e9b2785","executionInfo":{"status":"ok","timestamp":1584455482525,"user_tz":-540,"elapsed":11161,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["correct = 0\n","total = 0\n","net.eval()\n","preds = []\n","with torch.no_grad():\n","    for data in testdataloader:\n","        inputs = data\n","        inputs = torch.tensor(inputs, device=device).float()\n","        outputs = net(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        for pred in predicted:\n","          preds.append(pred.cpu().numpy())"],"execution_count":117,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sEXp5wukYC7x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1d40f288-2b6b-4fcc-d0ba-c8c12c0838b9","executionInfo":{"status":"ok","timestamp":1584455486314,"user_tz":-540,"elapsed":1156,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["preds = np.array(preds)\n","print(preds.shape)"],"execution_count":118,"outputs":[{"output_type":"stream","text":["(18000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oUJn6oaeYGJp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"a9a90d14-942e-4995-d949-243d5381a8f5","executionInfo":{"status":"ok","timestamp":1584455487923,"user_tz":-540,"elapsed":843,"user":{"displayName":"갱갱","photoUrl":"","userId":"06173154439249073995"}}},"source":["sample_submission[\"Category\"] = pd.Series(preds)\n","sample_submission.head()"],"execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>57808</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4960</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>35755</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>15543</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>48968</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Id  Category\n","0  57808         8\n","1   4960         0\n","2  35755         5\n","3  15543         3\n","4  48968         8"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"code","metadata":{"id":"dzqJJb8GYIs3","colab_type":"code","colab":{}},"source":["sample_submission.to_csv(\"torch2_submission.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdwK0-W56b3Y","colab_type":"text"},"source":["도대체...CNN안쓰고 성능을 0.99까지 어떻게 끌어올린것인지들,,, 우수과제 코드를 보고 공부해야겠다"]}]}